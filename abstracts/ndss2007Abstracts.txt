 Compared to attacks against end hosts, Denial of Ser- vice (DoS) attacks against the Internet infrastructure such as those targeted at routers can be more devastating due to their global impact on many networks. We discover that the recently identi(cid:2)ed low-rate TCP-targeted DoS attacks can have severe impact on the Border Gateway Protocol (BGP). As the interdomain routing protocol on today’s Internet, BGP is the critical infrastructure for exchanging reacha- bility information across the global Internet. We demon- strate empirically that BGP routing sessions on the current commercial routers are susceptible to such low-rate attacks launched remotely, leading to session resets and delayed routing convergence, seriously impacting routing stability and network reachability. This is a result of a fundamen- tal weakness with today’s deployed routing protocols: there is often no protection in the form of guaranteed bandwidth for routing traf(cid:2)c. Using testbed and Internet experiments, we thoroughly study the effect of such attacks on BGP. We demonstrate the feasibility of launching the attack in a coor- dinated fashion from wide-area hosts with arbitrarily low- rate individual attack (cid:3)ows, further raising the dif(cid:2)culty of detection. We explore defense solutions by protecting rout- ing traf(cid:2)c using existing router support. Our (cid:2)ndings high- light the importance of protecting the Internet infrastruc- ture, in particular control plane packets.  1 
 Encouraging the release of network data is central to promoting sound network research practices, though the publication of this data can leak sensitive information about the publishing organization. To address this dilemma, sev- eral techniques have been suggested for anonymizing net- work data by obfuscating sensitive ﬁelds. In this paper, we present new techniques for inferring network topology and deanonymizing servers present in anonymized network data, using only the data itself and public information. Via analyses on three different network datasets, we quantify the effectiveness of our techniques, showing that they can uncover signiﬁcant amounts of sensitive information. We also discuss prospects for preventing these deanonymiza- tion attacks.  1 
 Fingerprinting is a widely used technique among the net- working and security communities for identifying different implementations of the same piece of networking software running on a remote host. A ﬁngerprint is essentially a set of queries and a classiﬁcation function that can be applied on the responses to the queries in order to classify the software into classes. So far, identifying ﬁngerprints remains largely an arduous and manual process. This paper proposes a novel approach for automatic ﬁngerprint generation, that automatically explores a set of candidate queries and ap- plies machine learning techniques to identify the set of valid queries and to learn an adequate classiﬁcation function. Our results show that such an automatic process can gener- ate accurate ﬁngerprints that classify each piece of software into its proper class and that the search space for query ex- ploration remains largely unexploited, with many new such queries awaiting discovery. With a preliminary exploration, we are able to identify new queries not previously used for ﬁngerprinting.  1. 
 Cross-site scripting (XSS) is an attack against web ap- plications in which scripting code is injected into the output of an application that is then sent to a user’s web browser. In the browser, this scripting code is executed and used to transfer sensitive data to a third party (i.e., the attacker). Currently, most approaches attempt to prevent XSS on the server side by inspecting and modifying the data that is ex- changed between the web application and the user. Un- fortunately, it is often the case that vulnerable applications are not ﬁxed for a considerable amount of time, leaving the users vulnerable to attacks. The solution presented in this paper stops XSS attacks on the client side by tracking the ﬂow of sensitive information inside the web browser. If sen- sitive information is about to be transferred to a third party, the user can decide if this should be permitted or not. As a result, the user has an additional protection layer when surﬁng the web, without solely depending on the security of the web application.  1 
Forum spamming has become a major means of search engine spamming. To evaluate the impact of forum spamming on search quality, we have conducted a comprehensive study from three perspectives: that of the search user, the spammer, and the forum hosting site. We examine spam blogs and spam comments in both legitimate and honey forums. Our study shows that forum spamming is a widespread problem. Spammed forums, powered by the most popular software, show up in the top 20 search results for all the 189 popular keywords. On two blog sites, more than half (75% and 54% respectively) of the blogs are spam, and even on a major and reputably well maintained blog site, 8.1% of the blogs are spam1 . The observation on our honey forums confirms that spammers target abandoned pages and that most comment spam is meant to increase page rank rather than generate immediate traffic. We propose contextbased analyses, consisting of redirection and cloaking analysis, to detect spam automatically and to overcome shortcomings of content-based analyses. Our study shows that these analyses are very effective in identifying spam pages.
There are currently dozens of freely available tools to combat phishing and other web-based scams, many of which are web browser extensions that warn users when they are browsing a suspected phishing site. We developed an automated test bed for testing antiphishing tools. We used 200 verified phishing URLs from two sources and 516 legitimate URLs to test the effectiveness of 10 popular anti-phishing tools. Only one tool was able to consistently identify more than 90% of phishing URLs correctly; however, it also incorrectly identified 42% of legitimate URLs as phish. The performance of the other tools varied considerably depending on the source of the phishing URLs. Of these remaining tools, only one correctly identified over 60% of phishing URLs from both sources. Performance also changed significantly depending on the freshness of the phishing URLs tested. Thus we demonstrate that the source of phishing URLs and the freshness of the URLs tested can significantly impact the results of anti-phishing tool testing. We also demonstrate that many of the tools we tested were vulnerable to simple exploits. In this paper we describe our anti-phishing tool test bed, summarize our findings, and offer observations about the effectiveness of these tools as well as ways they might be improved.
 The trend toward smaller botnets may be more danger- ous than large botnets, in terms of large-scale attacks like distributed denials of service. We examine the possibility of “super-botnets,” networks of independent botnets that can be coordinated for attacks of unprecedented scale. For an adversary, super-botnets would also be extremely ver- satile and resistant to countermeasures. As such, super- botnets must be examined by the research community, so that defenses against this threat can be developed proac- tively. Our simulation results shed light on the feasibility and structure of super-botnets and some properties of their command-and-control mechanism. New forms of attack that super-botnets can launch are explored, and possible de- fenses against the threat of super-botnets are suggested.  1 
 A popular approach to detecting and characterizing threats such as worms and botnets involves the use of sac- riﬁcial host collections called honeynets. These collections are explicitly deployed to be scanned, compromised, and used in attacks. Unfortunately, existing approaches to de- ploying honeynets largely ignore the problem of conﬁgur- ing operating systems and applications on individual hosts, leaving the user to conﬁgure them in a manual and often ad hoc fashion. In this paper, we demonstrate that such ad hoc conﬁgurations are inadequate–they misrepresent the secu- rity landscape of the networks they are trying to protect and are relatively easy for attackers to discover. We show that manually building conﬁgurations with good visibility and resistance to discovery is hard, as each network has its own unique threat and vulnerability spaces, and the potential number of hosts to conﬁgure in the honeynet is quite large. Therefore, we need automated systems to assist network and security administrators in building honeynet conﬁgurations. We argue that honeynets with individually consistent hosts and proportional representation of the network will achieve the two desired goals of visibility into network attacks and resistance to discovery. We develop an automated technique based on proﬁling the network and random sampling to gen- erate these honeynet conﬁgurations. Through experimental evaluation and deployment of conﬁgurations generated by our technique, we demonstrate signiﬁcantly more visibility and higher resistance to discovery than current methods.  1 
 We present a method to implement consumable creden- tials in a logic-based distributed authorization system. Such credentials convey use-limited authority (e.g., to open a door once) or authority to utilize resources that are them- selves limited (e.g., concert tickets). We design and imple- ment mechanisms to enforce the consumption of credentials in a distributed system, and to protect credentials from non- productive consumption as might result from misbehavior or failure. We explain how these mechanisms can be used to support a distributed authorization system that uses a linear access-control logic. Finally, we give several usage exam- ples in the framework, and evaluate the performance of our implementation for use in a ubiquitous computing deploy- ment at our institution.  1. 
The need for communication privacy over public networks is of growing concern in today’s society. As a result, privacy-preserving authentication and key exchange protocols have become critical primitives in building secure distributed systems. Secret handshakes provide such a service by allowing two members of the same group to secretly and privately authenticate to each other and agree on a shared key for further communication. This paper presents the first efficient secret handshake schemes with unlinkable, reusable credentials that do not rely on random oracles for their security (solving open problems from prior literature). In previous work, secret handshakes were extended with roles, so that a group member A can specify the role another group member B must have in order to successfully complete the protocol with A. We generalize the traditional and role-based secret handshake in two ways. First, we present a secret handshake with dynamic matching, in which each party can specify both the group and the role the other must have in order to complete the handshake. Second, we provide a novel extension of secret handshakes to include attributes, allowing the handshake to be based on approximate (or fuzzy) matching. We demonstrate the practicality and efficiency of our protocols by evaluating a prototype implementation. We integrate our dynamic matching protocol into IPsec, and we detail the performance tradeoffs associated with our fuzzy matching scheme. Our experiments indicate that our solutions offer attractive performance.
 With Hidden Credentials Alice can send policy- encrypted data to Bob in such a way that he can decrypt the data only with the right combination of credentials. Alice gains no knowledge of Bob’s credentials in the process, and hence the name “Hidden Credentials.” Research on Hidden Credential systems has focused on messages sent to single recipients, where the sender needs to know the recipient’s pseudonym beforehand, and on Hidden Policies, where Bob learns as little information as possible about Alice’s pol- icy for decrypting the message. Current schemes provide weak policy privacy — with non-interactive schemes, the recipient can learn parts of the policy, and with interac- tive schemes based on secure multiparty computation, a user can try different sets of credentials as input to gain knowledge of the policy after repeated decryption attempts. Furthermore, existing schemes do not support policies with negations efﬁciently. For example, a policy stating “Bob is not a student” is hard to enforce since Bob can simply withhold, or not use, his student credential.  We propose a system called PEAPOD (Privacy- Enhanced Attribute-based Publishing Of Data) that pro- vides the following properties: (1) Users can securely pub- lish data protected by attribute-based policies to multiple possible recipients without requiring interaction between senders and receivers. This is achieved by using a semi- trusted server. (2) The plaintext message and the policy are completely hidden from the server. (3) Any recipient, intended or not, learns no other information about a mes- sage’s policy beyond the number of clauses in policy that were satisﬁed. Furthermore the recipient is forced to use all of his or her issued credentials for decryption, and there- fore cannot mount inference attacks by trying to decrypt the  ∗This research was supported in part by the NSF, under grant CNS- 0524695, and the Bureau of Justice Assistance, under grant 2005-DD-BX- 1091. The views and conclusions do not necessarily reﬂect the views of the sponsors.  (4) Lastly, message with different subsets of credentials. since recipients are forced to use all their credentials for decryption, PEAPOD efﬁciently supports non-monotonic boolean policies by allowing senders to include negations in their policies.  1. 
 This  paper  describes  a  system  that  supports  high availability of data, until the data should be expunged, at which time it is impossible to recover the data. This design  supports  three  types of  assured  delete;  expira- tion time known at file creation, on-demand deletion of individual  files,  and  custom  keys  for  classes  of  data. The obvious approach, of course, is to encrypt the data on  nonvolatile  storage,  and  then  destroy  keys  at  the appropriate  times.  However,  managing  ephemeral keys; robustly keeping them for some amount of time, and then reliably destroying every copy, is difficult. We partition the problem so that the burden of ephemeral key  management  can  be  outsourced  to  a  minimally trusted  third  party  we  refer  to  as  an  “ephemerizer”, with  negligible  performance  overhead,  resulting  in  a file system that is easy and inexpensive to manage.  1. 
 A direct recording electronic (DRE) voting machine must satisfy several requirements to ensure voter privacy and the integrity of the election. A recent proposal for a vote stor- age system due to Molnar et al. provides tamper-evidence properties while maintaining voter privacy by storing bal- lots on a programmable, read-only memory (PROM). We achieve the same properties and protect against additional threats of memory replacement through cryptographic tech- niques, without the use of special hardware. Our approach is based on a new cryptographic primitive called History- Hiding Append-Only Signatures.  1  
 We explore the limits of single-server computational pri- vate information retrieval (PIR) for the purpose of preserv- ing client access patterns leakage. We show that deployment of non-trivial single server PIR protocols on real hardware of the recent past would have been orders of magnitude less time-efﬁcient than trivially transferring the entire database. We stress that these results are beyond existing knowledge of mere “impracticality” under unfavorable assumptions. They rather reﬂect an inherent limitation with respect to modern hardware, likely the result of a communication-cost centric protocol design. We argue that this is likely to hold on non-specialized traditional hardware in the foreseeable future. We validate our reasoning in an experimental setup on modern off-the-shelf hardware. Ultimately, we hope our results will stimulate practical designs.  1 
 against integer-based attacks.  We present the design and implementation of RICH (Run-time Integer CHecking), a tool for efﬁciently detecting integer-based attacks against C programs at run time. C integer bugs, a popular avenue of attack and frequent pro- gramming error [1–15], occur when a variable value goes out of the range of the machine word used to materialize it, e.g. when assigning a large 32-bit int to a 16-bit short. We show that safe and unsafe integer operations in C can be captured by well-known sub-typing theory. The RICH compiler extension compiles C programs to object code that monitors its own execution to detect integer-based attacks. We implemented RICH as an extension to the GCC compiler and tested it on several network servers and UNIX utilities. Despite the ubiquity of integer operations, the performance overhead of RICH is very low, averaging about 5%. RICH found two new integer bugs and caught all but one of the previously known bugs we tested. These results show that RICH is a useful and lightweight software testing tool and run-time defense mechanism. RICH may generate false pos- itives when programmers use integer overﬂows deliberately, and it can miss some integer bugs because it does not model certain C features.  1 
The Shield project relied on application protocol analyzers to detect potential exploits of application vulnerabilities. We present the design of a second-generation generic application-level protocol analyzer (GAPA) that encompasses a domain-specific language and the associated run-time. We designed GAPA to satisfy three important goals: safety, real-time analysis and response, and rapid development of analyzers. We have found that these goals are relevant for many network monitors that implement protocol analysis. Therefore, we built GAPA to be readily integrated into tools such as Ethereal as well as Shield. GAPA preserves safety through the use of a memorysafe language for both message parsing and analysis, and through various techniques to reduce the amount of state maintained in order to avoid denial-of-service attacks. To support online analysis, the GAPA runtime uses a streamprocessing model with incremental parsing. In order to speed protocol development, GAPA uses a syntax similar to many protocol RFCs and other specifications, and incorporates many common protocol analysis tasks as built-in abstractions. We have specified 10 commonly used protocols in the GAPA language and found it expressive and easy to use. We measured our GAPA prototype and found that it can handle an enterprise client HTTP workload at up to 60 Mbps, sufficient performance for many end-host firewall/IDS scenarios. At the same time, the trusted code base of GAPA is an order of magnitude smaller than Ethereal.
The overall efficiency, reliability, and availability of a firewall is crucial in enforcing and administrating security, especially when the network is under attack. The continuous growth of the Internet, coupled with the increasing sophistication of the attacks, is placing stringent demands on firewall performance. These challenges require new designs, architecture and algorithms to optimize firewalls. In this paper, we propose OPTWALL, an adaptive hierarchical firewall optimization framework aimed at reducing operational cost of firewalls. The main features of the proposed approach are the hierarchical design, splitting techniques, an online traffic adaptation mechanism, and a strong reactive scheme to counter malicious attacks (e.g. Denial-of-Service (DoS) attacks). To the best of our knowledge, this work is the first of its kind to use traffic characteristics in the design of an adaptive hierarchical firewall optimization framework. To study the performance of OPTWALL, a set of experiments are conducted on Linux ipchains. The performance evaluation study uses a large set of firewall policies and traffic traces managed by a Tier1 ISP and provides security access for the ISP network from/to its business partners. Results show the high potential of OPTWALL to reduce the operational cost of firewalls. In particular, the results show that a performance improvement of nearly 35% can been achieved in a heavily loaded network environment.
