The  emergence of connections between telecommunications networks and the  Internet creates significant avenues for exploitation. For example,  through the use of small volumes of targeted traffic, researchers have  demonstrated a number of attacks capable of denying service to users in  major metropolitan areas. While such investigations have explored the  impact of specific vulnerabilities, they neglect to address a larger  issue - how the architecture of cellular networks makes these systems  susceptible to denial of service attacks. As we show in this paper,  these problems have little to do with a mismatch of available bandwidth.  Instead, they are the result of the pairing of two networks built on  fundamentally opposing design philosophies. We support this a claim by  presenting two new attacks on cellular data services. These attacks are  capable of preventing the use of high-bandwidth cellular data services  throughout an area the size of Manhattan with less than 200Kbps of  malicious traffic. We then examine the characteristics common to these  and previous attacks as a means of explaining why such vulnerabilites  are artifacts of design rigidity. Specifically, we show that the  shoehorning of data communications protocols onto a network rigorously  optimized for the delivery of voice causes that network to fail under  modest loads.
The growing  popularity of wireless networks and mobile devices is starting to  attract unwanted attention especially as potential targets for malicious  activities reach critical mass. In this study, we try to quantify the  threat from large-scale distributed attacks on wireless networks, and,  more specifically, wifi networks in densely populated metropolitan  areas. We focus on three likely attack scenarios: “wildfire” worms that  can spread contagiously over and across wireless LANs, coordinated  citywide phishing campaigns based on wireless spoofing, and rogue  systems for compromising location privacy in a coordinated fashion. The  first attack illustrates how dense wifi deployment may provide  opportunities for attackers who want to quickly compromise large numbers  of machines. The last two attacks illustrate how botnets can amplify  wifi vulnerabilities, and how botnet power is amplified by wireless  connectivity.   To quantify  these threats, we rely on real-world data extracted from wifi maps of  large metropolitan areas in the States and Singapore. Our results  suggest that a carefully crafted wireless worm can infect up to 80% of  all wifi connected hosts in some metropolitan areas within 20 minutes,  and that an attacker can launch phishing attacks or build a tracking  system to monitor the location of 10-50% of wireless users in these  metropolitan areas with just 1,000 zombies under his control.
Anonymization of network traces is widely viewed as a necessary   condition for releasing such data for research purposes.  For   obvious privacy reasons, an important goal of trace anonymization is   to suppress the recovery of web browsing activities.  While several   studies have examined the possibility of reconstructing web    browsing activities from anonymized packet-level traces, we argue   that these approaches fail to account for a number of challenges   inherent in real-world network traffic, and more so, are unlikely to   be successful on coarser NetFlow logs.     By contrast, we develop new approaches that identify    target web pages within anonymized NetFlow data, and address many    real-world challenges, such as browser caching and session    parsing.     We evaluate the effectiveness of our techniques in identifying    front pages from the 50 most popular web sites on the Internet (as ranked by alexa.com), in    both a closed-world experiment similar to that of earlier work and    in tests with real network flow logs.     Our results show that certain types of web pages with unique and    complex structure remain identifiable despite the use of state-of-the-art    anonymization techniques.  The concerns raised herein pose a threat to web    browsing privacy insofar as the attacker can approximate the web    browsing conditions represented in the flow logs.
Reverse  engineering of software is the process of recovering higher-level  structure and meaning from a lowerlevel program representation. It can  be used for legitimate purposes — e.g., to recover source code that has  been lost — but it is often used for nefarious purposes, e.g., to search  for security vulnerabilities in binaries or to steal intellectual  property. This paper addresses the problem of making it hard to reverse  engineering binary programs bymaking it difficult to disassemblemachine  code statically. Binaries are obfuscated by changing many control  transfers into signals (traps) and inserting dummy control transfers and  “junk” instructions after the signals. The resulting code is still a  correct program, but even the best current disassemblers are unable to  disassemble 40%–60% of the instructions in the program. Furthermore, the  disassemblers have a mistaken understanding of over half of the control  flow edges. However, the obfuscated program necessarily executes more  slowly than the original. Experimental results quantify the degree of  obfuscation, stealth of the code, and effects on execution time and code  size.
We introduce the first active hardware metering  scheme that aims to protect integrated circuits (IC) intellectual  property (IP) against piracy and runtime tampering. The novel metering  method simultaneously employs inherent unclonable variability in modern manufacturing technology, and functionality preserving alternations  of the structural IC specifications. Active metering works by enabling  the designers to lock each IC and to remotely disable it. The objectives  are realized by adding new states and transitions to the original  finite state machine (FSM) to create boosted finite state machines(BFSM)  of the pertinent design. A unique and unpredictable ID generated by an  IC is utilized to place an BFSM into the power-up state upon  activation. The designer, knowing the transition table, is the only one  who can generate input sequences required to bring the BFSM into the functional initial (reset) state. To facilitate remote disabling of ICs, black hole states are integrated within the BFSM.   We introduce nine types of potential attacks  against the proposed active metering method. We further describe a  number of countermeasures that must be taken to preserve the security of  active metering against the potential attacks. The implementation  details of the method with the objectives of being low-overhead,  unclonable, obfuscated, stable, while having a diverse set of keys is  presented. The active metering method was implemented, synthesized and  mapped on the standard benchmark circuits. Experimental evaluations  illustrate that the method has a low-overhead in terms of power, delay,  and area, while it is extremely resilient against the considered  attacks.
In this paper we describe bugs and ways to attack trusted 	computing systems based on a static root of trust such as 	Microsoft's Bitlocker.  We propose to use the dynamic root of 	trust feature of newer x86 processors as this shortens the 	trust chain, can minimize the Trusted Computing Base of 	applications and is less vulnerable to TPM and BIOS attacks. 	To support our claim we implemented the Open Secure LOader 	(OSLO), the first publicly available bootloader based on AMDs 	skinit instruction.
We describe  a “cheat” attack, allowing an ordinary process to hijack any desirable  percentage of the CPU cycles without requiring superuser/administrator  privileges. Moreover, the nature of the attack is such that, at least in  some systems, listing the active processes will erroneously show the  cheating process as not using any CPU resources: the “missing” cycles  would either be attributed to some other process or not be reported at  all (if the machine is otherwise idle). Thus, certain malicious  operations generally believed to have required overcoming the hardships  of obtaining root access and installing a rootkit, can actually be  launched by non-privileged users in a straightforward manner, thereby  making the job of a malicious adversary that much easier. We show that  most major general-purpose operating systems are vulnerable to the cheat  attack, due to a combination of how they account for CPU usage and how  they use this information to prioritize competing processes.  Furthermore, recent scheduler changes attempting to better support  interactive workloads increase the vulnerability to the attack, and  naive steps taken by certain systems to reduce the danger are easily  circumvented. We show that the attack can nevertheless be defeated, and  we demonstreate this by implementing a patch for Linux that eliminates  the problem with negligible overhead.
We are entering the multi-core era in computer science. All major  high-performance processor manufacturers have integrated at least two  cores (processors) on the same chip — and it is predicted that chips  with many more cores will become widespread in the near future. As  cores on the same chip share the DRAM memory system, multiple programs  executing on different cores can interfere with each others' memory  access requests, thereby adversely affecting one another's  performance.     In this paper, we demonstrate that current multi-core processors are  vulnerable to a new class of Denial of Service (DoS) attacks because  the memory system is “unfairly” shared among multiple cores. An  application can maliciously destroy the memory-related performance of  another application running on the same chip. We call such an  application a memory performance hog (MPH). With the widespread  deployment of multi-core systems in commodity desktop and laptop  computers, we expect MPHs to become a prevalent security issue that  could affect almost all computer users.     We show that an MPH can reduce the performance of another application  by 2.9 times in an existing dual-core system, without being  significantly slowed down itself; and this problem will become more  severe as more cores are integrated on the same chip. Our analysis  identifies the root causes of unfairness in the design of the memory  system that make multi-core processors vulnerable to MPHs. As a  solution to mitigate the performance impact of MPHs, we propose a new  memory system architecture that provides fairness to different  applications running on the same chip. Our evaluations show that this  memory system architecture is able to effectively contain the negative  performance impact of MPHs in not only dual-core but also 4-core and  8-core systems.
In this  paper we propose two new constructions for protecting the integrity of  files in cryptographic file systems. Our constructions are designed to  exploit two characteristics of many file-system workloads, namely low  entropy of file contents and high sequentiality of file block writes. At  the same time, our approaches maintain the best features of the most  commonly used algorithm today (Merkle trees), including defense against  replay of stale (previously overwritten) blocks and a small, constant  amount of trusted storage per file. Via implementations in the EncFS  cryptographic file system, we evaluate the performance and storage  requirements of our new constructions compared to those of Merkle trees.  We conclude with guidelines for choosing the best integrity algorithm  depending on typical application workload.
Application-level  protocol specifications are useful for many security applications,  including intrusion prevention and detection that performs deep packet  inspection and traffic normalization, and penetration testing that  generates network inputs to an application to uncover potential  vulnerabilities. However, current practice in deriving protocol  specifications is mostly manual. In this paper, we present Discoverer, a  tool for automatically reverse engineering the protocol message formats  of an application from its network trace. A key property of Discoverer  is that it operates in a protocol-independent fashion by inferring  protocol idioms commonly seen in message formats of many  application-level protocols. We evaluated the efficacy of Discoverer  over one text protocol (HTTP) and two binary protocols (RPC and  CIFS/SMB) by comparing our inferred formats with true formats obtained  from Ethereal [5]. For all three protocols, more than 90% of our  inferred formats correspond to exactly one true format; one true format  is reflected in five inferred formats on average; our inferred formats  cover over 95% of messages, which belong to 30-40% of true formats  observed in the trace.
Different implementations of the same protocol specification usually contain deviations, i.e., differences in how they check and process some of their inputs. Deviations are commonly introduced as implementation errors or as different interpretations of the same specification. Automatic discovery of these deviations is important for several applications. In this paper, we focus on automatic discovery of deviations for two particular applications: error detection and fingerprint generation. We propose a novel approach for automatically detecting deviations in the way different implementations of the same specification check and process their input. Our approach has several advantages: (1) by automatically building symbolic formulas from the implementation, our approach is precisely faithful to the implementation; (2) by solving formulas created from two different implementations of the same specification, our approach significantly reduces the number of inputs needed to find deviations; (3) our approach works on binaries directly, without access to the source code.  We have built a prototype implementation of our approach and have evaluated it using multiple implementations of two different protocols: HTTP and NTP. Our results show that our approach successfully finds deviations between different implementations, including errors in input checking, and differences in the interpretation of the specification, which can be used as fingerprints.
Unsolicited bulk e-mail, or SPAM, is a means to an end.  For virtually  all such messages, the intent is to attract the recipient into  entering a commercial transaction -- typically via a linked Web site.  While the prodigious infrastructure used to pump out billions of such  solicitations is essential, the engine driving this process is  ultimately the ``point-of-sale'' -- the various money-making ``scams''  that extract value from Internet users.  In the hopes of better  understanding the business pressures exerted on spammers, this paper  focuses squarely on the Internet infrastructure used to host and  support such scams.  We describe an opportunistic measurement  technique called spamscatter that mines emails in real-time,  follows the embedded link structure and automatically clusters the  destination Web sites using image shingling to capture graphical  similarity between rendered sites.  We have implemented this approach  on a large real-time spam feed (over 1M messages per week) and have  identified and analyzed over 2,000 distinct scams on 7,000 distinct  servers.
E-mail has  become indispensable in today’s networked society. However, the huge and  ever-growing volume of spam has become a serious threat to this  important communication medium. It not only affects e-mail recipients,  but also causes a significant overload to mail servers which handle the  e-mail transmission.  We perform  an extensive analysis of IP addresses and IP aggregates given by  network-aware clusters in order to investigate properties that can  distinguish the bulk of the legitimate mail and spam. Our analysis  indicates that the bulk of the legitimate mail comes from long-lived IP  addresses. We also find that the bulk of the spam comes from network  clusters that are relatively long-lived. Our analysis suggests that  network-aware clusters may provide a good aggregation scheme for  exploiting the history and structure of IP addresses.  We then  consider the implications of this analysis for prioritizing legitimate  mail. We focus on the situation when mail server is overloaded, and the  goal is to maximize the legitimate mail that it accepts. We demonstrate  that the history and the structure of the IP addresses can reduce the  adverse impact of mail server overload, by increasing the number of  legitimate e-mails accepted by a factor of 3.
We present a new kind of network perimeter monitoring strategy, which focuses on recognizing the infection and coordination dialog that occurs during a successful malware infection.  BotHunter is an application designed to track the two-way communication flows between internal assets and external entities, developing an evidence trail of data exchanges that match a state-based infection sequence model. BotHunter consists of a correlation engine that is driven by three malware-focused network packet sensors, each charged with detecting specific stages of the malware infection process, including inbound scanning, exploit usage, egg downloading, outbound bot coordination dialog, and outbound attack propagation.  The BotHunter correlator then ties together the dialog trail of inbound intrusion alarms with those outbound communication patterns that are highly indicative of successful local host infection.  When a sequence of evidence is found to match BotHunter's infection dialog model, a consolidated report is produced to capture all the relevant events and event sources that played a role during the infection process.  We refer to this analytical strategy of matching the dialog flows between internal assets and the broader Internet as dialog-based correlation, and contrast this strategy to other intrusion detection and alert correlation methods.  We present our experimental results using BotHunter in both virtual and live testing environments, and discuss our Internet release of the BotHunter prototype.  BotHunter is made available both for operational use and to help stimulate research in understanding the life cycle of malware infections.
Modern  smartcards, capable of sophisticated cryptography, provide a high  assurance of tamper resistance and are thus commonly used in payment  applications.  Although extracting secrets out of smartcards requires resources beyond  the means of many would-be thieves, the manner in which they are used  can be exploited for fraud.   Cardholders authorize financial transactions by presenting the card and  disclosing a PIN to a terminal without any assurance as to the amount  being charged or who is to be paid, and have no means of discerning  whether the terminal is authentic or not.  Even the most advanced smartcards cannot protect customers from being  defrauded by the simple relaying of data from one location to another.  We describe the development of such an attack, and show results from  live experiments on the UK's EMV implementation, Chip & PIN.  We discuss previously proposed defences, and show that these cannot  provide the required security assurances. A new defence based on a distance bounding protocol is described and  implemented, which requires only modest alterations to current hardware  and software. As far as we are aware, this is the first complete design and  implementation of a secure distance bounding protocol. Future smartcard generations could use this design to provide  cost-effective resistance to relay attacks, which are a genuine threat  to deployed applications. We also discuss the security-economics impact to customers of enhanced  authentication mechanisms.
Although  motivated by both usability and security concerns, the existing  literature on click-based graphical password schemes using a single  background image (e.g., PassPoints) has focused largely on usability. We  examine the security of such schemes, including the impact of different  background images, and strategies for guessing user passwords. We  report on both short- and long-term user studies: one lab-controlled,  involving 43 users and 17 diverse images, and the other a field test of  223 user accounts. We provide empirical evidence that popular points  (hot-spots) do exist for many images, and explore two different types of  attack to exploit this hot-spotting: (1) a “human-seeded” attack based  on harvesting click-points from a small set of users, and (2) an  entirely automated attack based on image processing techniques.  Our most effective attacks are generated by harvesting password data  from a small set of users to attack other targets. These attacks can guess 36% of user passwords within 231 guesses (or 12% within 216 guesses) in one instance, and 20% within 233 guesses (or 10% within 218  guesses) in a second instance. We perform an image-processing attack by  implementing and adapting a bottom-up model of visual attention, resulting in a purely automated tool that can guess up to 30% of user  passwords  in 235 guesses for some instances, but under 3% on others.  Our results suggest that these graphical password schemes appear to be at least as susceptible to offline attack  as the traditional text passwords they were proposed to replace.
We revisit  the venerable question of “pure password”- based key derivation and  encryption, and expose security weaknesses in current implementations  that stem from structural flaws in Key Derivation Functions (KDF). We  advocate a fresh redesign, named Halting KDF (HKDF), which we thoroughly  motivate on these grounds:  By letting password owners choose the hash iteration count, we gain operational flexibility and eliminate the rapid obsolescence faced by many existing schemes.   By throwing a Halting-Problem wrench in the works of guessing that iteration count, we widen the security gap with any attacker to its theoretical optimum.   By parallelizing the key derivation, we let legitimate users exploit all the computational power they can muster, which in turn further raises the bar for attackers.   HKDFs are  practical and universal: they work with any password, any hardware, and a  minor change to the user interface. As a demonstration, we offer  realworld implementations for the TrueCrypt and GnuPG packages, and  discuss their security benefits in concrete terms.
Voice over IP (VoIP) has become a popular protocol for making phone calls over the Internet.  Due to the potential transit of sensitive conversations over untrusted network infrastructure, it is well understood that the contents of a VoIP session should be encrypted. However, we demonstrate that current cryptographic techniques do not provide adequate protection when the underlying audio is encoded using bandwidth-saving Variable Bit Rate (VBR) coders.  Explicitly, we use the length of encrypted VoIP packets to tackle the challenging task of identifying the language of the conversation. Our empirical analysis of 2,066 native speakers of 21 different languages shows that a substantial amount of information can be discerned from encrypted VoIP traffic.  For instance, our 21-way classifier achieves 66% accuracy, almost a 14-fold improvement over  random guessing.  For 14 of the 21 languages, the accuracy is greater than 90%.   We achieve an overall binary classification (e.g., "Is this a  Spanish or English conversation?") rate of 86.6%.  Our analysis  highlights what we believe to be interesting new privacy issues in VoIP.
Newly  published data, when combined with existing public knowledge, allows for  complex and sometimes unintended inferences. We propose semi-automated  tools for detecting these inferences prior to releasing data. Our tools  give data owners a fuller understanding of the implications of releasing  data and help them adjust the amount of data they release to avoid  unwanted inferences.  Our tools  first extract salient keywords from the private data intended for  release. Then, they issue search queries for documents that match  subsets of these keywords, within a reference corpus (such as the public  Web) that encapsulates as much of relevant public knowledge as  possible. Finally, our tools parse the documents returned by the search  queries for keywords not present in the original private data. These  additional keywords allow us to automatically estimate the likelihood of  certain inferences. Potentially dangerous inferences are flagged for  manual review.   We call  this new technology Web-based inference control. The paper reports on  two experiments which demonstrate early successes of this technology.  The first experiment shows the use of our tools to automatically  estimate the risk that an anonymous document allows for  re-identification of its author. The second experiment shows the use of  our tools to detect the risk that a document is linked to a sensitive  topic. These experiments, while simple, capture the full complexity of  inference detection and illustrate the power of our approach.
We analyze three new consumer electronic gadgets in order to gauge the privacy and security trends in mass-market UbiComp devices. Our study of the Slingbox Pro uncovers a new information leakage vector for encrypted streaming multimedia. By exploiting properties of variable bitrate encoding schemes, we show that a passive adversary can determine with high probability the movie that a user is watching via her Slingbox, even when the Slingbox uses encryption. We experimentally evaluated our method against a database of over 100 hours of network traces for 26 distinct movies.  Despite an opportunity to provide significantly more location privacy than existing devices, like RFIDs, we find that an attacker can trivially exploit the Nike+iPod Sport Kit’s design to track users; we demonstrate this with a GoogleMaps-based distributed surveillance system. We also uncover security issues with the way Microsoft Zunes manage their social relationships.   We show how these products’ designers could have significantly raised the bar against some of our attacks. We also use some of our attacks to motivate fundamental security and privacy challenges for future UbiComp devices.
SIF  (Servlet Information Flow) is a novel software framework for building  high-assurance web applications, using language-based information-flow  control to enforce security. Explicit, end-to-end confidentiality and  integrity policies can be given either as compile-time program  annotations, or as run-time user requirements. Compile-time and run-time  checking efficiently enforce these policies. Information flow analysis  is known to be useful against SQL injection and cross-site scripting,  but SIF prevents inappropriate use of information more generally: the  flow of confidential information to clients is controlled, as is the  flow of low-integrity information from clients. Expressive policies  allow users and application providers to protect information from one  another.  SIF moves  trust out of the web application, and into the framework and compiler.  This provides application deployers with stronger security assurance.  Language-based  information flow promises cheap, strong information security. But until  now, it could not effectively enforce information security in highly  dynamic applications. To build SIF, we developed new language features  that make it possible to write realistic web applications. Increased  assurance is obtained with modest enforcement overhead.
We propose  new techniques to combat the problem of click fraud in pay-per-click  (PPC) systems. Rather than adopting the common approach of filtering out  seemingly fraudulent clicks, we consider instead an affirmative  approach that only accepts legitimate clicks, namely those validated  through client authentication. Our system supports a new advertising  model in which “premium” validated clicks assume higher value than  ordinary clicks of more uncertain authenticity. Click validation in our  system relies upon sites sharing evidence of the legitimacy of users  (distinguishing them from bots, scripts, or fraudsters). As cross-site  user tracking raises privacy concerns among many users, we propose ways  to make the process of authentication anonymous. Our premium-click  scheme is transparent to users. It requires no client-side changes and  imposes minimal overhead on participating Web sites.
This paper explores the use of execution-based Web content analysis to protect users from Internet-borne malware. Many anti-malware tools use signatures to identify malware infections on a user's PC. In contrast, our approach is to render and observe active Web content in a disposable virtual machine before it reaches the user's browser, identifying and blocking pages whose behavior is suspicious. Execution-based analysis can defend against undiscovered threats and zero-day attacks. However, our approach faces challenges, such as achieving good interactive performance, and limitations, such as defending against malicious Web content that contains non-determinism.To evaluate the potential for our execution-based technique, we designed, implemented, and measured a new proxy-based anti-malware tool called SpyProxy. SpyProxy intercepts and evaluates Web content in transit from Web servers to the browser. We present the architecture and design of our SpyProxy prototype, focusing in particular on the optimizations we developed to make on-the-fly execution-based analysis practical. We demonstrate that with careful attention to design, an execution-based proxy such as ours can be effective at detecting and blocking many of today's attacks while adding only small amounts of latency to the browsing experience. Our evaluation shows that SpyProxy detected every malware threat to which it was exposed, while adding only 600 milliseconds of latency to the start of page rendering for typical content.
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
No abstract
