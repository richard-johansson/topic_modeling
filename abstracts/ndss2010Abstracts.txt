 Online gaming is a lucrative and growing industry, but one that is slowed by cheating that compromises the gaming experience and hence drives away players (and revenues). In this paper we develop a technique by which game de- velopers can enable game operators to validate the behav- ior of game clients as being consistent with valid execu- tion of the sanctioned client software. Our technique em- ploys symbolic execution of the client software to extract constraints on client-side state implied by each client-to- server message, and then uses constraint solving to deter- mine whether the sequence of client-to-server messages can be “explained” by any possible user inputs, in light of the server-to-client messages already received. The requisite constraints and solving components can be developed ei- ther simultaneously with the game or retroactively for ex- isting games. We demonstrate our approach in two case studies: one of the open-source game XPilot, and one of a game similar to Pac-Man of our own design.  1 
Researchers at the University of Washington recently proposed Vanish [20], a system for creating messages that automatically “self-destruct” after a period of time. Vanish works by encrypting each message with a random key and storing shares of the key in a large, public distributed hash table (DHT). DHTs expunge data older than a certain age; after this happens to the key shares, the key is permanently lost, and the encrypted data is permanently unreadable. Vanish is an interesting approach to an important privacy problem, but, in its current form, it is insecure. In this paper, we defeat the deployed Vanish implementation, explain how the original paper’s security analysis is flawed, and draw lessons for future system designs. We present two Sybil attacks against the current Vanish implementation, which stores its encryption keys in the million-node Vuze BitTorrent DHT. These attacks work by continuously crawling the DHT and saving each stored value before it ages out. They can efficiently recover keys for more than 99% of Vanish messages. We show that the dominant cost of these attacks is network data transfer, not memory usage as the Vanish authors expected, and that the total cost is two orders of magnitude less than they estimated. While we consider potential defenses, we conclude that public DHTs like Vuze probably cannot provide strong security for Vanish.
We initiate study of the use of ‘secure tunnel’ protocols, specifically IPsec, and its availability and performance guarantees to higher-layer protocols, in particular TCP, against Denial/Degradation of Service (DoS) attacks. IPsec is designed to provide privacy and authentication against MITM attackers, and employs an anti-replay mechanism to ensure performance. For our analysis, we define a new family of adversaries, the stealth denial and degradation of service (DoS) adversaries. These adversaries are weaker than the classical MITM adversary, and may be of interest in other works. We analyse their ability to launch (DoS) attacks on secure channels, and show realistic amplification attacks, disrupting TCP communication over secure VPNs using IPsec. In particular, we show that antireplay mechanism is critical for performance by launching a DoS attack on communication over IPsec without antireplay window. We present attacks exploiting insufficient IPsec anti-replay window size, and show how to calculate correct window size. Finally we present attacks on IPsec with correctly adjusted anti-replay window size thus showing that even large anti-replay window does not ensure performance to TCP flows. We then suggest a fix to TCP in IPsec gateway designed to prevent the above attacks, and to provide secure channel immune to degradation and other DoS attacks. Our solution involves changes (only) to the sending gateway machines running IPsec. In addition to their practical importance, our results also raise the challenge of formally defining secure channels immune to DoS and degradation attacks, and providing provably-secure implementations.
 Browser extensions are remarkably popular, with one in three Firefox users running at least one extension. Although well-intentioned, extension developers are often not security experts and write buggy code that can be exploited by ma- licious web site operators. In the Firefox extension system, these exploits are dangerous because extensions run with the user’s full privileges and can read and write arbitrary ﬁles and launch new processes. In this paper, we analyze 25 popular Firefox extensions and ﬁnd that 88% of these extensions need less than the full set of available privileges. Additionally, we ﬁnd that 76% of these extensions use un- necessarily powerful APIs, making it difﬁcult to reduce their privileges. We propose a new browser extension system that improves security by using least privilege, privilege separa- tion, and strong isolation. Our system limits the misdeeds an attacker can perform through an extension vulnerabil- ity. Our design has been adopted as the Google Chrome extension system.  1  
 Online behavioral advertising (OBA) refers to the prac- tice of tracking users across web sites in order to infer user interests and preferences. These interests and preferences are then used for selecting ads to present to the user. There is great concern that behavioral advertising in its present form infringes on user privacy. The resulting public de- bate — which includes consumer advocacy organizations, professional associations, and government agencies — is premised on the notion that OBA and privacy are inherently in conﬂict.  In this paper we propose a practical architecture that enables targeting without compromising user privacy. Be- havioral proﬁling and targeting in our system takes place in the user’s browser. We discuss the effectiveness of the sys- tem as well as potential social engineering and web-based attacks on the architecture. One complication is billing; ad- networks must bill the correct advertiser without knowing which ad was displayed to the user. We propose an efﬁcient cryptographic billing system that directly solves the prob- lem. We implemented the core targeting system as a Firefox extension and report on its effectiveness.  1  
 The complexity of the client-side components of web applications has exploded with the increase in popularity of web 2.0 applications. Today, traditional desktop ap- plications, such as document viewers, presentation tools and chat applications are commonly available as online JavaScript applications.  Previous research on web vulnerabilities has primarily concentrated on ﬂaws in the server-side components of web applications. This paper highlights a new class of vulnera- bilities, which we term client-side validation (or CSV) vul- nerabilities. CSV vulnerabilities arise from unsafe usage of untrusted data in the client-side code of the web applica- tion that is typically written in JavaScript. In this paper, we demonstrate that they can result in a broad spectrum of attacks. Our work provides empirical evidence that CSV vulnerabilities are not merely conceptual but are prevalent in today’s web applications.  We propose dynamic analysis techniques to systemati- cally discover vulnerabilities of this class. The techniques are light-weight, efﬁcient, and have no false positives. We implement our techniques in a prototype tool called FLAX, which scales to real-world applications and has discovered 11 vulnerabilities in the wild so far.  1  
Learning-based anomaly detection has proven to be an effective black-box technique for detecting unknown attacks. However, the effectiveness of this technique crucially depends upon both the quality and the completeness of the training data. Unfortunately, in most cases, the traffic to the system (e.g., a web application or daemon process) protected by an anomaly detector is not uniformly distributed. Therefore, some components (e.g., authentication, payments, or content publishing) might not be exercised enough to train an anomaly detection system in a reasonable time frame. This is of particular importance in real-world settings, where anomaly detection systems are deployed with little or no manual configuration, and they are expected to automatically learn the normal behavior of a system to detect or block attacks. In this work, we first demonstrate that the features utilized to train a learning-based detector can be semantically grouped, and that features of the same group tend to induce similar models. Therefore, we propose addressing local training data deficiencies by exploiting clustering techniques to construct a knowledge base of well-trained models that can be utilized in case of undertraining. Our approach, which is independent of the particular type of anomaly detector employed, is validated using the realistic case of a learning-based system protecting a pool of web servers running several web applications such as blogs, forums, or Web services. We run our experiments on a real-world data set containing over 58 million HTTP requests to more than 36,000 distinct web application components. The results show that by using the proposed solution, it is possible to achieve effective attack detection even with scarce training data.
 Phishing websites, fraudulent sites that impersonate a trusted third party to gain access to private data, continue to cost Internet users over a billion dollars each year. In this paper, we describe the design and performance char- acteristics of a scalable machine learning classiﬁer we de- veloped to detect phishing websites. We use this classiﬁer to maintain Google’s phishing blacklist automatically. Our classiﬁer analyzes millions of pages a day, examining the URL and the contents of a page to determine whether or not a page is phishing. Unlike previous work in this ﬁeld, we train the classiﬁer on a noisy dataset consisting of mil- lions of samples from previously collected live classiﬁcation data. Despite the noise in the training data, our classiﬁer learns a robust model for identifying phishing pages which correctly classiﬁes more than 90% of phishing pages sev- eral weeks after training concludes.  1 
 The popularity of instant messaging (IM) services has recently attracted the interest of attackers that try to send malicious URLs or ﬁles to the contact lists of compro- mised instant messaging accounts or clients. This work focuses on a systematic characterization of IM threats based on the information collected by HoneyBuddy, a honeypot-like infrastructure for detecting malicious ac- tivities in IM networks. HoneyBuddy ﬁnds and adds contacts to its honeypot messengers by querying pop- ular search engines for IM contacts or by advertising its accounts on contact ﬁnder sites. Our deployment has shown that with over six thousand contacts we can gather between 50 and 110 malicious URLs per day as well as executables. Our experiments show that 21% of our collected executable samples were not gathered by other malware collection infrastructures, while 93% of the identiﬁed IM phishing domains were not recorded by popular blacklist mechanisms. Furthermore, our ﬁnd- ings show that the malicious domains are hosted by a limited number of hosts that remain practically un- changed throughout time.  1 
 IP-based blacklist is an effective way to ﬁlter spam emails. However, building and maintaining individual IP addresses in the blacklist is difﬁcult, as new mali- cious hosts continuously appear and their IP addresses may also change over time. To mitigate this problem, researchers have proposed to replace individual IP ad- dresses in the blacklist with IP clusters, e.g., BGP clus- ters. In this paper, we closely examine the accuracy of IP-cluster-based approaches to understand their effec- tiveness and fundamental limitations. Based on such understanding, we propose and implement a new clus- tering approach that considers both network origin and DNS information, and incorporate it with SpamAssas- sin, a popular spam ﬁltering system widely used today. Applying our approach to a 7-month email trace col- lected at a large university department, we can reduce the false negative rate by 50% compared with directly applying various public IP-based blacklists without in- creasing the false positive rate. Furthermore, using hon- eypot email accounts and real user accounts, we show that our approach can capture 30% - 50% of the spam emails that slip through SpamAssassin today.  1 
 Unsolicited bulk e-mail (UBE) or spam constitutes a sig- niﬁcant fraction of all e-mail connection attempts and rou- tinely frustrates users, consumes resources, and serves as an infection vector for malicious software. In an effort to scalably and effectively reduce the impact of these e-mails, e-mail system designers have increasingly turned to black- listing. Blacklisting (blackholing, block listing) is a form of course-grained, reputation-based, dynamic policy enforce- ment in which real-time feeds of spam sending hosts are sent to networks so that the e-mail from these hosts may be re- jected. Unfortunately, current spam blacklist services are highly inaccurate and exhibit both false positives and sig- niﬁcant false negatives. In this paper, we explore the root causes of blacklist inaccuracy and show that the trend to- ward stealthier spam exacerbates the existing tension be- tween false positives and false negatives when assigning spamming IP reputation. We argue that to relieve this ten- sion, global aggregation and reputation assignment should be replaced with local aggregation and reputation assign- ment, utilizing preexisting global spam collection, with the addition of local usage, policy, and reachability informa- tion. We propose two speciﬁc techniques based on this premise, dynamic thresholding and speculative aggregation, whose goal is to improve the accuracy of blacklist genera- tion. We evaluate the performance and accuracy of these solutions in the context of our own deployment consisting of 2.5 million production e-mails and 14 million e-mails from spamtraps deployed in 11 domains over a month-long pe- riod. We show that the proposed approaches signiﬁcantly improve the false positive and false negative rates when compared to existing approaches.  1  
 We have traditionally viewed spam from the receiver’s point of view: mail servers assaulted by a barrage of spam from which we must pick out a handful of legitimate mes- sages. In this paper we describe a system for better ﬁltering spam by exploiting the vantage point of the spammer. By instantiating and monitoring botnet hosts in a controlled environment, we are able to monitor new spam as it is cre- ated, and consequently infer the underlying template used to generate polymorphic e-mail messages. We demonstrate this approach on mail traces from a range of modern bot- nets and show that we can automatically ﬁlter such spam precisely and with virtually no false positives.  1. 
 We propose, develop, and implement techniques for achieving contractual anonymity. In contractual anon- ymity, a user and service provider enter into an anonym- ity contract. The user is guaranteed anonymity and mes- sage unlinkability from the contractual anonymity sys- tem unless she breaks the contract. The service provider is guaranteed that it can identify users who break the contract. The signiﬁcant advantages of our system are that 1) the service provider is not able to take any action toward a particular user (such as revealing her identity or blacklisting her future authentications) unless she vi- olates her contract, 2) our system can enforce a variety of policies, and 3) our system is efﬁcient.  1  
 This paper presents the design and implementation of Application-Aware Anonymity (A3), an extensible plat- form for deploying anonymity-based services on the In- ternet. A3 allows applications to tailor their anonymity properties and performance characteristics according to speciﬁc communication requirements.  To support ﬂexible path construction, A3 exposes a declarative language (A3LOG) that enables applications to compactly specify path selection and instantiation policies executed by a declarative networking engine. We demonstrate that our declarative language is sufﬁ- ciently expressive to encode novel multi-metric perfor- mance constraints as well as existing relay selection algorithms employed by Tor and other anonymity sys- tems, using only a few lines of concise code. We exper- imentally evaluate the A3 system using a combination of trace-driven simulations and deployment on Planet- Lab. Our experimental results demonstrate that A3 can ﬂexibly support a wide range of path selection and in- stantiation strategies at low performance overhead.  1  
 Random number generators (RNGs) are consistently a weak link in the secure use of cryptography. Routine cryp- tographic operations such as encryption and signing can fail spectacularly given predictable or repeated random- ness, even when using good long-lived key material. This has proved problematic in prior settings when RNG imple- mentation bugs, poor design, or low-entropy sources have resulted in predictable randomness. We investigate a new way in which RNGs fail due to reuse of virtual machine (VM) snapshots. We exhibit such VM reset vulnerabilities in widely-used TLS clients and servers: the attacker takes advantage of (or forces) snapshot replay to compromise sessions or even expose a server’s DSA signing key. Our next contribution is a backwards-compatible framework for hedging routine cryptographic operations against bad ran- domness, thereby mitigating the damage due to randomness failures. We apply our framework to the OpenSSL library and experimentally conﬁrm that it has little overhead.  1. 
Many modern software platforms today, including browsers, middleware server architectures, cell phone operating systems, web application engines, support thirdparty software extensions. This paper proposes InvisiType, an object-oriented approach that enables platform developers to efficiently enforce fine-grained safety checks on thirdparty extensions without requiring their cooperation. This allows us to harness the true power of third-party software by giving it access to sensitive data while ensuring that it does not leak data. In this approach, a platform developer encapsulates all safety checks in a policy class and selectively subjects objects at risk to these policies. The runtime enforces these policies simply by changing the types of these objects dynamically. It uses the virtual method dispatch mechanism to substitute the original methods and operations with code laced with safety checks efficiently. The runtime hides the type changes from application code so the original code can run unmodified. We have incorporated the notion of InvisiType into the Python language. We have applied the technique to 4 realworld Python web applications totaling 156,000 lines of code. InvisiType policies greatly enhance the security of the web applications, including MoinMoin, a popular, 94,000- line Wiki Engine. MoinMoin has a large number of thirdparty extensions, which makes security enforcement important. With less than 150 lines of Python code, we found 16 security bugs in MoinMoin. This represents a significant reduction in developers’ effort from a previous proposal, Flume, which required 1,000 lines of C++ code and modifications to 1,000 lines of Python code. Our InvisiType policies successfully found 19 cross-site scripting vulnerabilities and 6 access control errors in total. The overhead of applying the policies is less than 4 percent, indicating that the technique is practical. 
 Domain Name System Security Extensions (DNSSEC) with Hashed Authenticated Denial of Existence (NSEC3) is a protocol slated for adoption by important parts of the DNS hierarchy, including the root zone, as a solution to DNS security vulnerabilities such as “cache-poisoning” attacks. We study the security goals and operation of DNSSEC/NSEC3 and use Murϕ, a ﬁnite-state enumera- tion tool, to analyze its security guarantees and shortcom- ings. By checking DNSSEC/NSEC3 security properties in the presence of a network attacker, we uncover several weaknesses in the DNSSEC protocol, including incorrect temporal dependencies in the DNSSEC signature attesta- tion chain and NSEC3 options that allow a forged name to be inserted into a DNSSEC domain. We demonstrate the exploitability of the NSEC3 vulnerability by a browser cookie-stealing attack on a realistic laboratory DNSSEC domain. We offer implementation and conﬁguration advice which minimize the exploitability of the uncovered vulnera- bilities. After re-incorporating the advised repairs into the Murϕ DNSSEC model, we demonstrate the updated proto- col no longer contains vulnerabilities exploitable within our threat model.  1  
 Enterprise policy management is challenging and error- prone. Compared to existing work that focused on analyz- ing misconﬁgurations, our work is the ﬁrst to address the issues that arose during policy deployment, i.e., effecting policy changes. In this paper, we demonstrate that naive approaches to policy deployment can easily create secu- rity vulnerabilities, such as granting access of sensitive re- sources to unprivileged users or temporarily allowing mali- cious trafﬁc to critical network infrastructure. To systemat- ically solve this problem, we formally deﬁne secure and in- secure intermediate states, and further propose an efﬁcient algorithm to ﬁnd a deployment procedure without insecure intermediate states. We implemented and evaluated our al- gorithm on Group Policy framework, while only harnessing existing support and requiring no modiﬁcation to the cur- rent infrastructure. Our evaluation shows that our solution adds minimal overhead to the overall deployment time while provably eliminating insecure intermediate states.  1 
We analyze filename-based privilege escalation attacks, where an attacker creates filesystem links, thereby “tricking” a victim program into opening unintended files. We develop primitives for a POSIX environment, providing assurance that files in “safe directories” (such as /etc/passwd) cannot be opened by looking up a file by an “unsafe pathname” (such as a pathname that resolves through a symbolic link in a world-writable directory). In today’s UNIX systems, solutions to this problem are typically built into (some) applications and use applicationspecific knowledge about (un)safety of certain directories. In contrast, we seek solutions that can be implemented in the filesystem itself (or a library on top of it), thus providing protection to all applications. Our solution is built around the concept of pathname manipulators, which are roughly the users that can influence the result of a file lookup operation. For each user, we distinguish unsafe pathnames from safe pathnames according to whether or not the pathname has any manipulators other than that user or root. We propose a safe-open procedure that keeps track of the safety of the current pathname as it resolves it, and that takes extra precautions while opening files with unsafe pathnames. We prove that our solution can prevent a common class of filename-based privilege escalation attacks, and describe our implementation of the safe-open procedure as a library function over the POSIX filesystem interface. We tested our implementation on several UNIX variants to evaluate its implications for systems and applications. Our experiments suggest that this solution can be deployed in a portable way without breaking existing systems, and that it is effective against this class of pathname resolution attacks.
 We present Joe-E, a language designed to support the development of secure software systems. Joe-E is a subset of Java that makes it easier to architect and implement pro- grams with strong security properties that can be checked during a security review. It enables programmers to ap- ply the principle of least privilege to their programs; imple- ment application-speciﬁc reference monitors that cannot be bypassed; introduce and use domain-speciﬁc security ab- stractions; safely execute and interact with untrusted code; and build secure, extensible systems. Joe-E demonstrates how it is possible to achieve the strong security properties of an object-capability language while retaining the fea- tures and feel of a mainstream object-oriented language. Additionally, we present ways in which Java’s static type safety complements object-capability analysis and permits additional security properties to be veriﬁed statically, com- pared with previous object-capability languages which rely on runtime checks. In this paper, we describe the design and implementation of Joe-E and its advantages for secu- rity and auditability over standard Java. We demonstrate how Joe-E can be used to develop systems with novel secu- rity properties that would be difﬁcult or impossible to en- sure otherwise, including a web application platform that provides transparent, transactional object persistence and can safely host multiple mutually-distrustful applications in a single JVM.  1  
 Publishers wish to sandbox third-party advertisements to protect themselves from malicious advertisements. One promising approach, used by ADsafe, Dojo Secure, and Jacaranda, sandboxes advertisements by statically verify- ing that their JavaScript conforms to a safe subset of the language. These systems blacklist known dangerous proper- ties that would let advertisements escape the sandbox. Un- fortunately, this approach does not prevent advertisements from accessing new methods added to the built-in prototype objects by the hosting page. In this paper, we show that one- third of the Alexa US Top 100 web sites would be exploitable by an ADsafe-veriﬁed advertisement. We propose an im- proved statically veriﬁed JavaScript subset that whitelists known-safe properties using namespaces. Our approach maintains the expressiveness and performance of static ver- iﬁcation while Binary code reuse is the process of automatically identifying the interface and extracting the instructions and data dependencies of a code fragment from an executable program, so that it is self-contained and can be reused by external code. Binary code reuse is useful for a number of security applications, including reusing the proprietary cryptographic or unpacking functions from a malware sample and for rewriting a network dialog. In this paper we conduct the first systematic study of automated binary code reuse and its security applications. The main challenge in binary code reuse is understanding the code fragment’s interface. We propose a novel technique to identify the prototype of an undocumented code fragment directly from the program’s binary, without access to source code or symbol information. Further, we must also extract the code itself from the binary so that it is self-contained and can be easily reused in another program. We design and implement a tool that uses a combination of dynamic and static analysis to automatically identify the prototype and extract the instructions of an assembly function into a form that can be reused by other C code. The extracted function can be run independently of the rest of the program’s functionality and shared with other users. We apply our approach to scenarios that include extracting the encryption and decryption routines from malware samples, and show that these routines can be reused by a network proxy to decrypt encrypted traffic on the network. This allows the network proxy to rewrite the malware’s encrypted traffic by combining the extracted encryption and decryption functions with the session keys and the protocol grammar. We also show that we can reuse a code fragment from an unpacking function for the unpacking routine for a different sample of the same family, even if the code fragment is not a complete function 
 With only the binary executable of a program,  it is useful to discover the program’s data structures and infer their syntactic and semantic deﬁnitions. Such knowledge is highly valuable in a variety of security and forensic applica- tions. Although there exist efforts in program data structure inference, the existing solutions are not suitable for our targeted application scenarios. In this paper, we propose a reverse engineering technique to automatically reveal program data structures from binaries. Our technique, called REWARDS, is based on dynamic analysis. More speciﬁcally, each memory location accessed by the program is tagged with a timestamped type attribute. Following the program’s runtime data ﬂow, this attribute is propagated to other memory locations and registers that share the same type. During the propagation, a variable’s type gets resolved if it is involved in a type-revealing execution point or “type sink”. More importantly, besides the forward type propagation, REWARDS involves a backward type resolution procedure where the types of some previously accessed variables get recursively resolved starting from a type sink. This procedure is constrained by the timestamps of relevant memory locations to disambiguate variables re- using the same memory location. In addition, REWARDS is able to reconstruct in-memory data structure layout based on the type information derived. We demonstrate that REWARDS provides unique beneﬁts to two applications: memory image forensics and binary fuzzing for vulnerabil- ity discovery.  1  
 can efﬁciently detect malware samples that use a variety of techniques to identify emulated analysis environments.  Malware is the root cause of many security threats on the Internet. To cope with the thousands of new malware samples that are discovered every day, security compa- nies and analysts rely on automated tools to extract the runtime behavior of malicious programs. Of course, mal- ware authors are aware of these tools and increasingly try to thwart their analysis techniques. To this end, mal- ware code is often equipped with checks that look for ev- idence of emulated or virtualized analysis environments. When such evidence is found, the malware program be- haves differently or crashes, thus showing a different “personality” than on a real system.  Recent work has introduced transparent analysis plat- forms (such as Ether or Cobra) that make it signif- icantly more difﬁcult for malware programs to detect their presence. Others have proposed techniques to iden- tify and bypass checks introduced by malware authors. Both approaches are often successful in exposing the runtime behavior of malware even when the malicious code attempts to thwart analysis efforts. However, these techniques induce signiﬁcant performance overhead, es- pecially for ﬁne-grained analysis. Unfortunately, this makes them unsuitable for the analysis of current high- volume malware feeds.  In this paper, we present a technique that efﬁciently detects when a malware program behaves differently in an emulated analysis environment and on an uninstru- mented reference host. The basic idea is simple: we just compare the runtime behavior of a sample in our anal- ysis system and on a reference machine. However, ob- taining a robust and efﬁcient comparison is very difﬁcult. In particular, our approach consists of recording the in- teractions of the malware with the operating system in one run and using this information to deterministically replay the program in our analysis environment. Our ex- periments demonstrate that, by using our approach, one  1  
Malware is the root cause of many security threats on the Internet. To cope with the thousands of new malware samples that are discovered every day, security companies and analysts rely on automated tools to extract the runtime behavior of malicious programs. Of course, malware authors are aware of these tools and increasingly try to thwart their analysis techniques. To this end, malware code is often equipped with checks that look for evidence of emulated or virtualized analysis environments. When such evidence is found, the malware program behaves differently or crashes, thus showing a different “personality” than on a real system. Recent work has introduced transparent analysis platforms (such as Ether or Cobra) that make it significantly more difficult for malware programs to detect their presence. Others have proposed techniques to identify and bypass checks introduced by malware authors. Both approaches are often successful in exposing the runtime behavior of malware even when the malicious code attempts to thwart analysis efforts. However, these techniques induce significant performance overhead, especially for fine-grained analysis. Unfortunately, this makes them unsuitable for the analysis of current highvolume malware feeds. In this paper, we present a technique that efficiently detects when a malware program behaves differently in an emulated analysis environment and on an uninstrumented reference host. The basic idea is simple: we just compare the runtime behavior of a sample in our analysis system and on a reference machine. However, obtaining a robust and efficient comparison is very difficult. In particular, our approach consists of recording the interactions of the malware with the operating system in one run and using this information to deterministically replay the program in our analysis environment. Our experiments demonstrate that, by using our approach, one can efficiently detect malware samples that use a variety of techniques to identify emulated analysis environments.