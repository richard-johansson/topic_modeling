 Authenticating broadcast packet communications poses a challenge that cannot be addressed e–ciently with public key signatures on each packet, or securely with the use of a pre-distributed shared secret key, or practically with unicast tunnels. Unreliability is an intrinsic problem: many broadcast protocols assume that some information will be lost, making it problem- atic to amortize the cost of a single public key signa- ture across multiple packets. Forward Error Correction (FEC) can compensate for loss of packets, but denial of service risks prevent the naive use of both public keys and FEC in authentication. In this paper we in- troduce a protocol, Broadcast Authentication Streams (BAS), that overcomes these barriers and provides a simple and e–cient scheme for authenticating broad- cast packet communications based on a new technique called selective veriﬂcation. We analyze BAS theoreti- cally, experimentally, and architecturally.  1 
 We introduce distillation codes, a method for streaming and storing data. Like erasure codes, distillation codes allow information to be decoded from a sufﬁciently large quorum of symbols. In contrast to erasure codes, distilla- tion codes are robust against pollution attacks, a powerful class of denial of service (DoS) attacks in which adver- saries inject invalid symbols during the decoding process. We examine applications of distillation codes to mul- ticast authentication. Previous applications of erasure codes to multicast authentication are vulnerable to low bandwidth pollution attacks. We demonstrate pollution attacks against previous approaches which prevent re- ceivers from verifying any authentic packets. To resist pol- lution attacks, we introduce Pollution Resistant Authenti- cated Block Streams, which have low overhead and can tolerate arbitrary patterns of packet loss within a block up to a predetermined number of packets. In the face of 40Mb/s of attack trafﬁc, PRABS receivers successfully au- thenticate the stream and consume only 10% of their CPU.  1. 
 Model-based intrusion detection compares a process’s ex- ecution against a program model to detect intrusion at- tempts. Models constructed from static program analy- sis have historically traded precision for ef(cid:2)ciency. We address this problem with our Dyck model, the (cid:2)rst ef(cid:2)- cient statically-constructed context-sensitive model. This model speci(cid:2)es both the correct sequences of system calls that a program can generate and the stack changes oc- curring at function call sites. Experiments demonstrate that the Dyck model is an order of magnitude more pre- cise than a context-insensitive (cid:2)nite state machine model. With null call squelching, a dynamic technique to bound cost, the Dyck model operates in time similar to the context- insensitive model.  We also present two static analysis techniques designed to counter mimicry and evasion attacks. Our branch anal- ysis identi(cid:2)es between 32% and 64% of our test programs’ system call sites as affecting control (cid:3)ow via their return values. Interprocedural argument capture of general val- ues recovers 32% to 69% more arguments than previously reported techniques.  1. 
Sharing data between widely distributed intrusion detection systems offers the possibility of significant improvements in speed and accuracy over isolated systems. In this paper, we describe and evaluate DOMINO (Distributed Overlay for Monitoring InterNet Outbreaks); an architecture for a distributed intrusion detection system that fosters collaboration among heterogeneous nodes organized as an overlay network. The overlay design enables DOMINO to be heterogeneous, scalable, and robust to attacks and failures. An important component of DOMINO’s design is the use of active-sink nodes which respond to and measure connections to unused IP addresses. This enables efficient detection of attacks from spoofed IP sources, reduces false positives, enables attack classification and production of timely blacklists. We evaluate the capabilities and performance of DOMINO using a large set of intrusion logs collected from over 1600 providers across the Internet. Our analysis demonstrates the significant marginal benefit obtained from distributed intrusion data sources coordinated through a system like DOMINO. We also evaluate how to configure DOMINO in order to maximize performance gains from the perspectives of blacklist length, blacklist freshness and IP proximity. We perform a retrospective analysis on the 2002 SQL-Snake and 2003 SQL-Slammer epidemics that highlights how information exchange through DOMINO would have reduced the reaction time and false-alarm rates during outbreaks. Finally, we provide preliminary results from our prototype active-sink deployment that illustrates the limited variability in the sink traffic and the feasibility of
 Several alert correlation methods were proposed in the past several years to construct high-level attack scenarios from low-level intrusion alerts reported by intrusion de- tection systems (IDSs). These correlation methods have different strengths and limitations; none of them clearly dominate the others. However, all of these methods de- pend heavily on the underlying IDSs, and perform poorly when the IDSs miss critical attacks. In order to improve the performance of intrusion alert correlation and reduce the impact of missed attacks, this paper presents a se- ries of techniques to integrate two complementary types of alert correlation methods: (1) those based on the sim- ilarity between alert attributes, and (2) those based on prerequisites and consequences of attacks. In particular, this paper presents techniques to hypothesize and reason about attacks possibly missed by IDSs based on the indi- rect causal relationship between intrusion alerts and the constraints they must satisfy. This paper also discusses additional techniques to validate the hypothesized attacks through raw audit data and to consolidate the hypothe- sized attacks to generate concise attack scenarios. The ex- perimental results in this paper demonstrate the potential of these techniques in building high-level attack scenarios and reasoning about possibly missed attacks.  1. 
 Web spooﬁng is a signiﬁcant problem involving fraud- ulent email and web sites that trick unsuspecting users into revealing private information. We discuss some aspects of common attacks and propose a framework for client-side defense: a browser plug-in that exam- ines web pages and warns the user when requests for data may be part of a spoof attack. While the plug- in, SpoofGuard, has been tested using actual sites ob- tained through government agencies concerned about the problem, we expect that web spooﬁng and other forms of identity theft will be continuing problems in coming years.  1 
Wormhole attacks enable an attacker with limited resources and no cryptographic material to wreak havoc on wireless networks. To date, no general defenses against wormhole attacks have been proposed. This paper presents an analysis of wormhole attacks and proposes a countermeasure using directional antennas. We present a cooperative protocol whereby nodes share directional information to prevent wormhole endpoints from masquerading as false neighbors. Our defense greatly diminishes the threat of wormhole attacks and requires no location information or clock synchronization.
 Since the days of the Morris worm, the spread of malicious code has been the most imminent menace to the Internet. Worms use various scanning methods to spread rapidly. Worms that select scan destinations carefully can cause more damage than worms employing random scan. This paper analyzes various scan tech- niques. We then propose a generic worm detection ar- chitecture that monitors malicious activities. We pro- pose and evaluate an algorithm to detect the spread of worms using real time traces and simulations. We ﬂnd that our solution can detect worm activities when only 4% of the vulnerable machines are infected. Our results bring insight on the future battle against worm attacks.  1  
 Despite previous efforts in auditing software manually and automatically, buffer overruns are still being discov- ered in programs in use. A dynamic bounds checker de- tects buffer overruns in erroneous software before it oc- curs and thereby prevents attacks from corrupting the in- tegrity of the system.  Dynamic buffer overrun detectors have not been adopted widely because they either (1) cannot guard against all buffer overrun attacks, (2) break existing code, or (3) incur too high an overhead. This paper presents a practical detector called CRED (C Range Error Detec- tor) that avoids each of these deﬁciencies. CRED ﬁnds all buffer overrun attacks as it directly checks for the bounds of memory accesses. Unlike the original referent-object based bounds-checking technique, CRED does not break existing code because it uses a novel solution to support program manipulation of out-of-bounds addresses. Fi- nally, by restricting the bounds checks to strings in a pro- gram, CRED’s overhead is greatly reduced without sacri- ﬁcing protection in the experiments we performed.  CRED is implemented as an extension of the GNU C compiler version 3.3.1. The simplicity of our design makes possible a robust implementation that has been tested on over 20 open-source programs, comprising over 1.2 million lines of C code. CRED proved effective in de- tecting buffer overrun attacks on programs with known vulnerabilities, and is the only tool found to guard against a testbed of 20 different buffer overﬂow attacks[34]. Find- ing overruns only on strings impose an overhead of less  This research was performed while the ﬁrst author was at Stanford Uni- versity, and this material is based upon work supported in part by the National Science Foundation under Grant No. 0086160.  than 26% for 14 of the programs, and an overhead of up to 130% for the remaining six, while the previous state-of- the-art bounds checker by Jones and Kelly breaks 60% of the programs and is 12 times slower. Incorporating well- known techniques for optimizing bounds checking into CRED could lead to further performance improvements.  1. 
 Host compromise is a serious computer security problem today. To better protect hosts, several Manda- tory Access Control systems, such as Security Enhanced Linux (SELinux) and AppArmor, have been introduced. In this paper we propose an approach to analyze and compare the quality of protection offered by these dif- ferent MAC systems. We introduce the notion of vul- nerability surfaces under attack scenarios as the mea- surement of protection quality, and implement a tool called VulSAN for computing such vulnerability sur- faces. In VulSAN, we encode security policies, system states, and system rules using logic programs. Given an attack scenario, VulSAN computes a host attack graph and the vulnerability surface. We apply our approach to compare SELinux and AppArmor policies in several Linux distributions and discuss the results. Our tool can also be used by Linux system administrators as a system hardening tool. Because of its ability to analyze SELinux as well as AppArmor policies, it can be used for most en- terprise Linux distributions and home user distributions.  1  
 Application sandboxes provide restricted execution en- vironments that limit an application’s access to sensitive OS resources. These systems are an increasingly popular method for limiting the impact of a compromise. While a variety of mechanisms for building these systems have been proposed, the most thoroughly implemented and studied are based on system call interposition. Current interposition- based architectures offer a wide variety of properties that make them an attractive approach for building sandbox- ing systems. Unfortunately, these architectures also possess several critical properties that make their implementation error prone and limit their functionality.  We present a study of Ostia, a sandboxing system we have developed that relies on a “delegating” architecture which overcomes many of the limitations of today’s sand- boxing systems. We compare this delegating architecture to the “ﬁltering” architecture commonly used for sandboxes today. We present the salient features of each architecture and examine the design choices that signiﬁcantly impact se- curity, compatibility, ﬂexibility, deployability, and perfor- mance in this class of system.  1  
 In the Outsourced Database (ODB) model, organiza- tions outsource their data management needs to an exter- nal service provider. The service provider hosts clients’ databases and offers seamless mechanisms to create, store, update and access (query) their databases. This model in- troduces several research issues related to data security. One of the core security requirements is providing ef(cid:2)cient mechanisms to ensure data integrity and authenticity while incurring minimal computation and bandwidth overhead. In this work, we investigate the problem of ensuring data in- tegrity and suggest secure and practical schemes that help facilitate authentication of query replies. We explore the ap- plicability of popular digital signature schemes (RSA and DSA) as well as a recently proposed scheme due to Boneh et al. [1] and present their performance measurements.  is a critical asset) at an external, potentially untrusted, Database Service Provider site. It is essential to provide adequate security measures to protect the stored data from both malicious outsider attacks and the Database Service Provider itself. Security in most part, implies maintaining data integrity and guarding data privacy. Although some work [3] has been done to protect data con(cid:2)dentiality while guaranteeing its continued availability to authorized users, the problem of providing ef(cid:2)cient integrity in this model has not received much attention.  In this paper, we focus on providing secure and effective means of ensuring data authentication and integrity, while incurring minimal computational and bandwidth overhead. In particular, we investigate techniques to help the ODB client authenticate the origin and verify the integrity of data returned by the service provider in response to a posed query.  1 
 Audit logs are an important part of any secure system, and they need to be carefully designed in order to give a faithful representation of past system activity. This is espe- cially true in the presence of adversaries who might want to tamper with the audit logs. While it is important that au- ditors can inspect audit logs to assess past system activity, the content of an audit log may contain sensitive informa- tion, and should therefore be protected from unauthorized parties.  Protecting the contents of audit logs from unauthorized parties (i.e., encrypting it), while making it efﬁciently searchable by authorized auditors poses a problem. We de- scribe an approach for constructing searchable encrypted audit logs which can be combined with any number of exist- ing approaches for creating tamper-resistant logs. In par- ticular, we implemented an audit log for database queries that uses hash chains for integrity protection and identity- based encryption with extracted keywords to enable search- ing on the encrypted log. Our technique for keyword search on encrypted data has wide application beyond searchable audit logs.  1. 
 Elliptic Curve Cryptography (ECC) is emerging as an attractive alternative to traditional public-key cryptosys- tems (RSA, DSA, DH). ECC offers equivalent security with smaller key sizes resulting in faster computations, lower power consumption, as well as memory and bandwidth sav- ings. While these characteristics make ECC especially ap- pealing for mobile devices, they can also alleviate the com- putational burden on secure web servers.  This article studies the performance impact of using ECC with SSL, the dominant Internet security protocol. We cre- ated an ECC-enhanced version of OpenSSL and used it to benchmark the Apache web server. Our results show that, under realistic workloads, an Apache web server can han- dle 13%–31% more HTTPS requests per second when using ECC-160 rather than RSA-1024 reﬂecting short-term secu- rity levels. At security levels necessary to protect data be- yond 2010, the use of ECC-224 over RSA-2048 improves server performance by 120%–279%.  1. 
 A number of applications have emerged over recent years that use datagram transport. These applications include real time video conferencing, Internet telephony, and online games such as Quake and StarCraft. These ap- plications are all delay sensitive and use unreliable data- gram transport. Applications that are based on reliable transport can be secured using TLS, but no compelling al- ternative exists for securing datagram based applications. In this paper we present DTLS, a datagram capable ver- sion of TLS. DTLS is extremely similar to TLS and there- fore allows reuse of pre-existing protocol infrastructure. Our experimental results show that DTLS adds minimal overhead to a previously non-DTLS capable application.  1. 
Version 4 of the widely deployed Kerberos authentica- tion protocol encrypts essential information without ade- quate authentication. We have implemented an efﬁcient chosen-plaintext attack that uses this design ﬂaw to im- personate arbitrary principals. Related ﬂaws exist in ver- sion 5 of the protocol. We discuss the mistakes in the design of the protocol that contribute to these vulnerabil- ities, and how to avoid making them. We identify correc- tive measures taken in the proposed revisions to version 5, which repair these ﬂaws.  1.  
