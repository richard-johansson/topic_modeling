Software Fault Isolation (SFI) is an effective approach to sandboxing binary code of questionable provenance, an interesting use case for native plugins in a Web browser. We present software fault isolation schemes for ARM and x86-64 that provide control-ﬂow and memory integrity with average performance overhead of under 5% on ARM and 7% on x86-64. We believe these are the best known SFI implementations for these architectures, with signiﬁcantly lower overhead than previous systems for similar architectures. Our experience suggests that these SFI implementations beneﬁt from instruction-level parallelism, and have particularly small impact for work- loads that are data memory-bound, both properties that tend to reduce the impact of our SFI systems for future CPU implementations.  
  UserFS provides egalitarian OS protection mechanisms in Linux. UserFS allows any user—not just the system administrator—to allocate Unix user IDs, to use chroot, and to set up ﬁrewall rules in order to conﬁne untrusted code. One key idea in UserFS is representing user IDs as ﬁles in a /proc-like ﬁle system, thus allowing applica- tions to manage user IDs like any other ﬁles, by setting permissions and passing ﬁle descriptors over Unix do- main sockets. UserFS addresses several challenges in making user IDs egalitarian, including accountability, re- source allocation, persistence, and UID reuse. We have ported several applications to take advantage of UserFS; by changing just tens to hundreds of lines of code, we prevented attackers from exploiting application-level vul- nerabilities, such as code injection or missing ACL checks in a PHP-based wiki application. Implementing UserFS requires minimal changes to the Linux kernel—a single 3,000-line kernel module—and incurs no performance overhead for most operations, making it practical to de- ploy on real systems.  1  
 Capsicum is a lightweight operating system capabil- ity and sandbox framework planned for inclusion in FreeBSD 9. Capsicum extends, rather than replaces, UNIX APIs, providing new kernel primitives (sandboxed capability mode and capabilities) and a userspace sand- box API. These tools support compartmentalisation of monolithic UNIX applications into logical applications, an increasingly common goal supported poorly by dis- cretionary and mandatory access control. We demon- strate our approach by adapting core FreeBSD utilities and Google’s Chromium web browser to use Capsicum primitives, and compare the complexity and robustness of Capsicum with other sandboxing techniques.  1  
 In a bid to limit the harm caused by ubiquitous remotely exploitable software vulnerabilities, the computer sys- tems security community has proposed primitives to al- low execution of application code with reduced privilege. In this paper, we identify and address the vital and largely unexamined problem of how to structure implementa- tions of cryptographic protocols to protect sensitive data despite exploits. As evidence that this problem is poorly understood, we ﬁrst identify two attacks that lead to disclosure of sensitive data in two published state-of- the-art designs for exploit-resistant cryptographic proto- col implementations: privilege-separated OpenSSH, and the HiStar/DStar DIFC-based SSL web server. We then describe how to structure protocol implementations on UNIX- and DIFC-based systems to defend against these two attacks and protect sensitive information from dis- closure. We demonstrate the practicality and generality of this approach by applying it to protect sensitive data in the implementations of both the server and client sides of OpenSSH and of the OpenSSL library.  1  
Current Electronic Toll Pricing (ETP) implementa- tions rely on on-board units sending ﬁne-grained loca- tion data to the service provider. We present PrETP, a privacy-preserving ETP system in which on-board units can prove that they use genuine data and perform cor- rect operations while disclosing the minimum amount of location data. PrETP employs a cryptographic proto- col, Optimistic Payment, which we deﬁne in the ideal- world/real-world paradigm, construct, and prove secure under standard assumptions. We provide an efﬁcient im- plementation of this construction and build an on-board unit on an embedded microcontroller which is, to the best of our knowledge, the ﬁrst self-contained prototype that supports remote auditing. We thoroughly analyze our system from a security, legal and performance perspec- tive and demonstrate that PrETP is suitable for low-cost commercial applications.  1  
We study the security and privacy of private browsing modes recently added to all major browsers. We ﬁrst pro- pose a clean deﬁnition of the goals of private browsing and survey its implementation in different browsers. We conduct a measurement study to determine how often it is used and on what categories of sites. Our results suggest that private browsing is used differently from how it is marketed. We then describe an automated technique for testing the security of private browsing modes and report on a few weaknesses found in the Firefox browser. Fi- nally, we show that many popular browser extensions and plugins undermine the security of private browsing. We propose and experiment with a workable policy that lets users safely run extensions in private browsing mode.  1  
 A key feature that distinguishes modern botnets from earlier counterparts is their increasing use of structured overlay topologies. This lets them carry out sophisticated coordinated activities while being resilient to churn, but In this it can also be used as a point of detection. work, we devise techniques to localize botnet mem- bers based on the unique communication patterns aris- ing from their overlay topologies used for command and control. Experimental results on synthetic topologies embedded within Internet trafﬁc traces from an ISP’s backbone network indicate that our techniques (i) can lo- calize the majority of bots with low false positive rate, and (ii) are resilient to incomplete visibility arising from partial deployment of monitoring systems and measure- ment inaccuracies from dynamics of background trafﬁc.  1  
Regular expression (RE) matching is a core component of deep packet inspection in modern networking and security devices. In this paper, we propose the ﬁrst hardware-based RE matching approach that uses Ternary Content Addressable Memories (TCAMs), which are off-the-shelf chips and have been widely deployed in modern networking devices for packet classiﬁcation. We propose three novel techniques to reduce TCAM space and improve RE matching speed: transition sharing, ta- ble consolidation, and variable striding. We tested our techniques on 8 real-world RE sets, and our results show that small TCAMs can be used to store large DFAs and achieve potentially high RE matching throughtput. For space, we were able to store each of the corresponding 8 DFAs with as many as 25,000 states in a 0.59Mb TCAM chip where the number of TCAM bits required per DFA state were 12, 12, 12, 13, 14, 26, 28, and 42. Using a different TCAM encoding scheme that facilitates pro- cessing multiple characters per transition, we were able to achieve potential RE matching throughputs of between 10 and 19 Gbps for each of the 8 DFAs using only a sin- gle 2.36 Mb TCAM chip.  1 
 Search engines not only assist normal users, but also pro- vide information that hackers and other malicious enti- ties can exploit in their nefarious activities. With care- fully crafted search queries, attackers can gather infor- mation such as email addresses and misconﬁgured or even vulnerable servers.  We present SearchAudit, a framework that identiﬁes malicious queries from massive search engine logs in or- der to uncover their relationship with potential attacks. SearchAudit takes in a small set of malicious queries as seed, expands the set using search logs, and generates regular expressions for detecting new malicious queries. For instance, we show that, relying on just 500 malicious queries as seed, SearchAudit discovers an additional 4 million distinct malicious queries and thousands of vul- nerable Web sites. In addition, SearchAudit reveals a series of phishing attacks from more than 400 phishing domains that compromised a large number of Windows Live Messenger user credentials. Thus, we believe that SearchAudit can serve as a useful tool for identifying and preventing a wide class of attacks in their early phases.  1  
 Web applications are the most common way to make ser- vices and data available on the Internet. Unfortunately, with the increase in the number and complexity of these applications, there has also been an increase in the num- ber and complexity of vulnerabilities. Current techniques to identify security problems in web applications have mostly focused on input validation ﬂaws, such as cross- site scripting and SQL injection, with much less attention devoted to application logic vulnerabilities.  Application logic vulnerabilities are an important class of defects that are the result of faulty application logic. These vulnerabilities are speciﬁc to the functionality of particular web applications, and, thus, they are extremely difﬁcult to characterize and identify. In this paper, we propose a ﬁrst step toward the automated detection of application logic vulnerabilities. To this end, we ﬁrst use dynamic analysis and observe the normal operation of a web application to infer a simple set of behavioral spe- ciﬁcations. Then, leveraging the knowledge about the typical execution paradigm of web applications, we ﬁlter the learned speciﬁcations to reduce false positives, and we use model checking over symbolic input to identify program paths that are likely to violate these speciﬁca- tions under speciﬁc conditions, indicating the presence of a certain type of web application logic ﬂaws. We de- veloped a tool, called Waler, based on our ideas, and we applied it to a number of web applications, ﬁnding previously-unknown logic vulnerabilities.  1  
 Maintaining correct access control to shared resources such as ﬁle servers, wikis, and databases is an important part of enterprise network management. A combination of many factors, including high rates of churn in organi- zational roles, policy changes, and dynamic information- sharing scenarios, can trigger frequent updates to user permissions, leading to potential inconsistencies. With Baaz, we present a distributed system that monitors up- dates to access control metadata, analyzes this informa- tion to alert administrators about potential security and accessibility issues, and recommends suitable changes. Baaz detects misconﬁgurations that manifest as small in- consistencies in user permissions that are different from what their peers are entitled to, and prevents integrity and conﬁdentiality vulnerabilities that could lead to insider attacks. In a deployment of our system on an organiza- tional ﬁle server that stored conﬁdential data, we found 10 high level security issues that impacted 1639 out of 105682 directories. These were promptly rectiﬁed.  1 
Use-after-free vulnerabilities exploiting so-called dan- gling pointers to deallocated objects are just as dangerous as buffer overﬂows: they may enable arbitrary code exe- cution. Unfortunately, state-of-the-art defenses against use-after-free vulnerabilities require compiler support, pervasive source code modiﬁcations, or incur high per- formance overheads. This paper presents and evaluates Cling, a memory allocator designed to thwart these at- tacks at runtime. Cling utilizes more address space, a plentiful resource on modern machines, to prevent type- unsafe address space reuse among objects of different types. It infers type information about allocated objects at runtime by inspecting the call stack of memory allo- cation routines. Cling disrupts a large class of attacks against use-after-free vulnerabilities, notably including those hijacking the C++ virtual function dispatch mecha- nism, with low CPU and physical memory overhead even for allocation intensive applications.  1  
 In recent years, many advances have been made in cryptography, as well as in the performance of commu- nication networks and processors. As a result, many ad- vanced cryptographic protocols are now efﬁcient enough to be considered practical, yet research in the area re- mains largely theoretical and little work has been done to use these protocols in practice, despite a wealth of po- tential applications.  This paper introduces a simple description language, ZKPDL, and an interpreter for this language. ZKPDL implements non-interactive zero-knowledge proofs of knowledge, a primitive which has received much atten- tion in recent years. Using our language, a single pro- gram may specify the computation required by both the prover and veriﬁer of a zero-knowledge protocol, while our interpreter performs a number of optimizations to lower both computational and space overhead.  Our motivating application for ZKPDL has been the efﬁcient implementation of electronic cash. As such, we have used our language to develop a cryptographic library, Cashlib, that provides an interface for using e- cash and fair exchange protocols without requiring ex- pert knowledge from the programmer.  1  
 In this paper we introduce a framework for privacy- preserving distributed computation that is practical for many real-world applications. The framework is called Peers for Privacy (P4P) and features a novel heteroge- neous architecture and a number of efﬁcient tools for performing private computation and ensuring security at large scale. It maintains the following properties: (1) Provably strong privacy; (2) Adequate efﬁciency at rea- sonably large scale; and (3) Robustness against realis- tic adversaries. The framework gains its practicality by decomposing data mining algorithms into a sequence of vector addition steps that can be privately evaluated us- ing a new veriﬁable secret sharing (VSS) scheme over small ﬁeld (e.g., 32 or 64 bits), which has the same cost as regular, non-private arithmetic. This paradigm sup- ports a large number of statistical learning algorithms in- cluding SVD, PCA, k-means, ID3, EM-based machine learning algorithms, etc., and all algorithms in the sta- tistical query model [36]. As a concrete example, we show how singular value decomposition (SVD), which is an extremely useful algorithm and the core of many data mining tasks, can be done efﬁciently with privacy in P4P. Using real-world data and actual implementation we demonstrate that P4P is orders of magnitude faster than existing solutions.  1 
 Secure multiparty computation (MPC) allows joint privacy-preserving computations on data of multiple par- ties. Although MPC has been studied substantially, building solutions that are practical in terms of compu- tation and communication cost is still a major challenge. In this paper, we investigate the practical usefulness of MPC for multi-domain network security and monitor- ing. We ﬁrst optimize MPC comparison operations for processing high volume data in near real-time. We then design privacy-preserving protocols for event correlation and aggregation of network trafﬁc statistics, such as ad- dition of volume metrics, computation of feature entropy, and distinct item count. Optimizing performance of par- allel invocations, we implement our protocols along with a complete set of basic operations in a library called SEPIA. We evaluate the running time and bandwidth re- quirements of our protocols in realistic settings on a lo- cal cluster as well as on PlanetLab and show that they work in near real-time for up to 140 input providers and 9 computation nodes. Compared to implementations us- ing existing general-purpose MPC frameworks, our pro- tocols are signiﬁcantly faster, requiring, for example, 3 minutes for a task that takes 2 days with general-purpose frameworks. This improvement paves the way for new applications of MPC in the area of networking. Finally, we run SEPIA’s protocols on real trafﬁc traces of 17 net- works and show how they provide new possibilities for distributed troubleshooting and early anomaly detection.  1 
 Many applications of IP geolocation can beneﬁt from ge- olocation that is robust to adversarial clients. These in- clude applications that limit access to online content to a speciﬁc geographic region and cloud computing, where some organizations must ensure their virtual machines stay in an appropriate geographic region. This paper studies the applicability of current IP geolocation tech- niques against an adversary who tries to subvert the tech- niques into returning a forged result. We propose and evaluate attacks on both delay-based IP geolocation tech- niques and more advanced topology-aware techniques. Against delay-based techniques, we ﬁnd that the adver- sary has a clear trade-off between the accuracy and the detectability of an attack. In contrast, we observe that more sophisticated topology-aware techniques actually fare worse against an adversary because they give the adversary more inputs to manipulate through their use of topology and delay information.  1 
 The Domain Name System (DNS) is an essential protocol used by both legitimate Internet applications and cyber at- tacks. For example, botnets rely on DNS to support agile com- mand and control infrastructures. An effective way to disrupt these attacks is to place malicious domains on a “blocklist” (or “blacklist”) or to add a ﬁltering rule in a ﬁrewall or net- work intrusion detection system. To evade such security coun- termeasures, attackers have used DNS agility, e.g., by using new domains daily to evade static blacklists and ﬁrewalls. In this paper we propose Notos, a dynamic reputation system for DNS. The premise of this system is that malicious, agile use of DNS has unique characteristics and can be distinguished from legitimate, professionally provisioned DNS services. No- tos uses passive DNS query data and analyzes the network and zone features of domains. It builds models of known legit- imate domains and malicious domains, and uses these models to compute a reputation score for a new domain indicative of whether the domain is malicious or legitimate. We have eval- uated Notos in a large ISP’s network with DNS trafﬁc from 1.4 million users. Our results show that Notos can identify malicious domains with high accuracy (true positive rate of 96.8%) and low false positive rate (0.38%), and can identify these domains weeks or even months before they appear in public blacklists.  1 
 On November 3, 2009, voters in Takoma Park, Mary- land, cast ballots for the mayor and city council members using the Scantegrity II voting system—the ﬁrst time any end-to-end (E2E) voting system with ballot privacy has been used in a binding governmental election. This case study describes the various efforts that went into the election—including the improved design and imple- mentation of the voting system, streamlined procedures, agreements with the city, and assessments of the experi- ences of voters and poll workers.  The election, with 1728 voters from six wards, in- volved paper ballots with invisible-ink conﬁrmation codes, instant-runoff voting with write-ins, early and absentee (mail-in) voting, dual-language ballots, provi- sional ballots, privacy sleeves, any-which-way scanning with parallel conventional desktop scanners, end-to-end veriﬁability based on optional web-based voter veriﬁca- tion of votes cast, a full hand recount, thresholded author- ities, three independent outside auditors, fully-disclosed software, and exit surveys for voters and pollworkers.  Despite some glitches, the use of Scantegrity II was a success, demonstrating that E2E cryptographic voting systems can be effectively used and accepted by the gen- eral public.  1  
We examine the problem of acoustic emanations of print- ers. We present a novel attack that recovers what a dot- matrix printer processing English text is printing based on a record of the sound it makes, if the microphone is close enough to the printer. In our experiments, the at- tack recovers up to 72 % of printed words, and up to 95 % if we assume contextual knowledge about the text, with a microphone at a distance of 10cm from the printer. After an upfront training phase, the attack is fully auto- mated and uses a combination of machine learning, au- dio processing, and speech recognition techniques, in- cluding spectrum features, Hidden Markov Models and linear classiﬁcation; moreover, it allows for feedback- based incremental learning. We evaluate the effective- ness of countermeasures, and we describe how we suc- cessfully mounted the attack in-ﬁeld (with appropriate privacy protections) in a doctor’s practice to recover the content of medical prescriptions.  1 
 Wireless networks are being integrated into the modern automobile. The security and privacy implications of such in-car networks, however, have are not well under- stood as their transmissions propagate beyond the con- (cid:2)nes of a car’s body. To understand the risks associated with these wireless systems, this paper presents a privacy and security evaluation of wireless Tire Pressure Moni- toring Systems using both laboratory experiments with isolated tire pressure sensor modules and experiments with a complete vehicle system. We show that eaves- dropping is easily possible at a distance of roughly 40m from a passing vehicle. Further, reverse-engineering of the underlying protocols revealed static 32 bit identi- (cid:2)ers and that messages can be easily triggered remotely, which raises privacy concerns as vehicles can be tracked through these identi(cid:2)ers. Further, current protocols do not employ authentication and vehicle implementations do not perform basic input validation, thereby allowing for remote spoo(cid:2)ng of sensor messages. We validated this experimentally by triggering tire pressure warning messages in a moving vehicle from a customized soft- ware radio attack platform located in a nearby vehicle. Finally, the paper concludes with a set of recommenda- tions for improving the privacy and security of tire pres- sure monitoring systems and other forthcoming in-car wireless sensor networks.  1 
 The browser has become the de facto platform for ev- eryday computation. Among the many potential attacks that target or exploit browsers, vulnerabilities in browser extensions have received relatively little attention. Cur- rently, extensions are vetted by manual inspection, which does not scale well and is subject to human error.  In this paper, we present VEX, a framework for high- lighting potential security vulnerabilities in browser ex- tensions by applying static information-ﬂow analysis to the JavaScript code used to implement extensions. We describe several patterns of ﬂows as well as unsafe pro- gramming practices that may lead to privilege escala- tions in Firefox extensions. VEX analyzes Firefox ex- tensions for such ﬂow patterns using high-precision, context-sensitive, ﬂow-sensitive static analysis. We an- alyze thousands of browser extensions, and VEX ﬁnds six exploitable vulnerabilities, three of which were previ- ously unknown. VEX also ﬁnds hundreds of examples of bad programming practices that may lead to security vul- nerabilities. We show that compared to current Mozilla extension review tools, VEX greatly reduces the human burden for manually vetting extensions when looking for key types of dangerous ﬂows.  1  
 Web browsers are increasingly designed to be ex- tensible to keep up with the Web’s rapid pace of change. This extensibility is typically implemented using script-based extensions. Script extensions have access to sensitive browser APIs and content from untrusted web pages. Unfortunately, this pow- erful combination creates the threat of privilege es- calation attacks that grant web page scripts the full privileges of extensions and control over the entire browser process.  This paper makes two contributions. First, it describes the pitfalls of script-based extensibility based on our study of the Firefox web browser. We ﬁnd that script-based extensions can lead to arbi- trary code injection and execution control, the same types of vulnerabilities found in unsafe code. Sec- ond, we propose a taint-based system to track the spread of untrusted data in the browser and to de- tect the characteristic signatures of privilege escala- tion attacks. We evaluate this approach by using ex- ploits from the Firefox bug database and show that our system detects the vast majority of attacks with almost no false alarms.  1  
 Web publishers frequently integrate third-party adver- tisements into web pages that also contain sensitive pub- lisher data and end-user personal data. This practice ex- poses sensitive page content to conﬁdentiality and in- tegrity attacks launched by advertisements. In this pa- per, we propose a novel framework for addressing security threats posed by third-party advertisements. The heart of our framework is an innovative isolation mechanism that enables publishers to transparently interpose between ad- vertisements and end users. The mechanism supports ﬁne- grained policy speciﬁcation and enforcement, and does not affect the user experience of interactive ads. Evalua- tion of our framework suggests compatibility with several mainstream ad networks, security from many threats from advertisements and acceptable performance overheads.  1  
One of the main obstacles for the wider deployment of radio (RF) distance bounding is the lack of plat- forms that implement these protocols. We address this problem and we build a prototype system that demonstrates that radio distance bounding protocols can be implemented to match the strict processing that these protocols require. Our system implements a prover that is able to receive, process and transmit signals in less than 1ns. The security guarantee that a distance bounding protocol built on top of this sys- tem therefore provides is that a malicious prover can, at most, pretend to be about 15cm closer to the ver- iﬁer than it really is. To enable such fast processing at the prover, we use specially implemented concate- nation as the prover’s processing function and show how it can be integrated into a distance bounding protocol. Finally, we show that functions such as XOR and the comparison function, that were used in a number of previously proposed distance bounding protocols, are not best suited for the implementation of radio distance bounding.  1  
 Today, Internet trafﬁc is encrypted only when deemed necessary. Yet modern CPUs could feasibly encrypt most trafﬁc. Moreover, the cost of doing so will only drop over time. Tcpcrypt is a TCP extension designed to make end-to-end encryption of TCP trafﬁc the default, not the exception. To facilitate adoption tcpcrypt provides back- wards compatibility with legacy TCP stacks and middle- boxes. Because it is implemented in the transport layer, it protects legacy applications. However, it also provides a hook for integration with application-layer authentica- tion, largely obviating the need for applications to en- crypt their own network trafﬁc and minimizing the need for duplication of functionality. Finally, tcpcrypt mini- mizes the cost of key negotiation on servers; a server us- ing tcpcrypt can accept connections at 36 times the rate achieved using SSL.  1  
 Despite the widespread deployment of malware- detection software, in many situations it is difﬁcult to preemptively block a malicious program from infecting a system. Rather, signatures for detection are usually available only after malware have started to infect a large Ideally, infected systems should be group of systems. reinstalled from scratch. However, due to the high cost of reinstallation, users may prefer to rely on the remedi- ation capabilities of malware detectors to revert the ef- fects of an infection. Unfortunately, current malware de- tectors perform this task poorly, leaving users’ systems in an unsafe or unstable state. This paper presents an architecture to automatically generate remediation pro- cedures from malicious programs—procedures that can be used to remediate all and only the effects of the mal- ware’s execution in any infected system. We have imple- mented a prototype of this architecture and used it to gen- erate remediation procedures for a corpus of more than 200 malware binaries. Our evaluation demonstrates that the algorithm outperforms the remediation capabilities of top-rated commercial malware detectors.  1  
 Reverse Turing tests, or CAPTCHAs, have become an ubiquitous defense used to protect open Web resources from being exploited at scale. An effective CAPTCHA resists existing mechanistic software solving, yet can be solved with high probability by a human being. In response, a robust solving ecosystem has emerged, re- selling both automated solving technology and real- time human labor to bypass these protections. Thus, CAPTCHAs can increasingly be understood and evaluated in purely economic terms; the market price of a solution vs the monetizable value of the asset being protected. We examine the market-side of this question in depth, ana- lyzing the behavior and dynamics of CAPTCHA-solving service providers, their price performance, and the un- derlying labor markets driving this economy.  1  
 Oppressive regimes and even democratic governments restrict Internet access. Existing anti-censorship systems often require users to connect through proxies, but these systems are relatively easy for a censor to discover and block. This paper offers a possible next step in the cen- sorship arms race: rather than relying on a single system or set of proxies to circumvent censorship ﬁrewalls, we explore whether the vast deployment of sites that host user-generated content can breach these ﬁrewalls. To ex- plore this possibility, we have developed Collage, which allows users to exchange messages through hidden chan- nels in sites that host user-generated content. Collage has two components: a message vector layer for embedding content in cover trafﬁc; and a rendezvous mechanism to allow parties to publish and retrieve messages in the cover trafﬁc. Collage uses user-generated content (e.g., photo-sharing sites) as “drop sites” for hidden messages. To send a message, a user embeds it into cover trafﬁc and posts the content on some site, where receivers retrieve this content using a sequence of tasks. Collage makes it difﬁcult for a censor to monitor or block these messages by exploiting the sheer number of sites where users can exchange messages and the variety of ways that a mes- sage can be hidden. Our evaluation of Collage shows that the performance overhead is acceptable for sending small messages (e.g., Web articles, email). We show how Collage can be used to build two applications: a direct messaging application, and a Web content delivery sys- tem.  1  
 Many techniques have been proposed to generate keys including text passwords, graphical passwords, biomet- ric data and etc. Most of these techniques are not resis- tant to coercion attacks in which the user is forcefully asked by an attacker to generate the key to gain access to the system or to decrypt the encrypted ﬁle. We present a novel approach in generating cryptographic keys to ﬁght against coercion attacks. Our novel technique in- corporates the user’s emotional status, which changes when the user is under coercion, into the key generation through measurements of the user’s skin conductance. We present a model that generates cryptographic keys with one’s voice and skin conductance. In order to ex- plore more, a preliminary user study with 39 subjects was done which shows that our approach has moderate false- positive and false-negative rates. We also present the at- tacker’s strategy in guessing the cryptographic keys, and show that the resulting change in the password space un- der such attacks is small.  1 
