 Cross-site scripting (or XSS) has been the most domi- nant class of web vulnerabilities in 2007. The main under- lying reason for XSS vulnerabilities is that web markup and client-side languages do not provide principled mechanisms to ensure secure, ground-up isolation of user-generated data in web application code. In this paper, we develop a new approach that combines randomization of web ap- plication code and runtime tracking of untrusted data both on the server and the browser to combat XSS attacks. Our technique ensures a fundamental integrity property that pre- vents untrusted data from altering the structure of trusted code throughout the execution lifetime of the web applica- tion. We call this property document structure integrity (or DSI). Similar to prepared statements in SQL, DSI enforce- ment ensures automatic syntactic isolation of inline user- generated data at the parser-level. This forms the basis for conﬁnement of untrusted data in the web browser based on a server-speciﬁed policy.  We propose a client-server architecture that enforces document structure integrity in a way that can be imple- mented in current browsers with a minimal impact to com- patibility and that requires minimal effort from the web de- veloper. We implemented a proof-of-concept and demon- strated that such DSI enforcement with a simple default pol- icy is sufﬁcient to defeat over 98% of the 5,328 real-world reﬂected XSS vulnerabilities documented in 2007, with very low performance overhead both on the client and server.  1 
 Over the past few years, injection vulnerabilities have become the primary target for remote exploits. SQL in- jection, command injection, and cross-site scripting are some of the popular attacks that exploit these vulnerabili- ties. Taint-tracking has emerged as one of the most promis- ing approaches for defending against these exploits, as it supports accurate detection (and prevention) of popular in- jection attacks. However, practical deployment of taint- tracking defenses has been hampered by a number of fac- tors, including: (a) high performance overheads (often over 100%), (b) the need for deep instrumentation, which has the potential to impact application robustness and stabil- ity, and (c) speciﬁcity to the language in which an appli- cation is written. In order to overcome these limitations, we present a new technique in this paper called taint infer- ence. This technique does not require any source-code or binary instrumentation of the application to be protected; instead, it operates by intercepting requests and responses from this application. For most web applications, this inter- ception may be achieved using network layer interposition or library interposition. We then develop a class of policies called syntax- and taint-aware policies that can accurately detect and/or block most injection attacks. An experimental evaluation shows that our techniques are effective in detect- ing a broad range of attacks on applications written in mul- tiple languages (including PHP, Java and C), and impose low performance overheads (below 5%).  1  
 Cross-site scripting (XSS) vulnerabilities are among the most common and serious web application vulnerabilities. Eliminating XSS is challenging because it is difﬁcult for web applications to sanitize all user inputs appropriately. We present Noncespaces, a technique that enables web clients to distinguish between trusted and untrusted content to pre- vent exploitation of XSS vulnerabilities. Using Nonces- paces, a web application randomizes the XML namespace preﬁxes of tags in each document before delivering it to the client. As long as the attacker is unable to predict the ran- domized preﬁxes, the client can distinguish between trusted content created by the web application and untrusted con- tent provided by an attacker. To implement Noncespaces with minimal changes to web applications, we leverage a popular web application architecture to automatically ap- ply Noncespaces to static content processed through a pop- ular PHP template engine. We show that with simple poli- cies Noncespaces thwarts popular XSS attack vectors.  1. 
 We introduce a new paradigm for outsourcing the dura- bility property of a multi-client transactional database to an untrusted service provider. Speciﬁcally, we enable un- trusted service providers to support transaction serializa- tion, backup and recovery for clients, with full data conﬁ- dentiality and correctness. Moreover, providers learn noth- ing about transactions (except their size and timing), thus achieving read and write access pattern privacy.  We build a proof-of-concept implementation of this pro- tocol for the MySQL database management system, achiev- ing tens of transactions per second in a two-client scenario with full transaction privacy and guaranteed correctness. This shows the method is ready for production use, creating a novel class of secure database outsourcing models.  1 
 Many existing privacy-preserving techniques for query- ing distributed databases of sensitive information do not scale for large databases due to the use of heavyweight cryptographic techniques. In addition, many of these proto- cols require several rounds of interactions between the par- ticipants which may be impractical in wide-area settings. At the other extreme, a trusted party based approach does provide scalability but it forces the individual databases to reveal private information to the central party.  This paper shows how to perform various privacy- preserving operations in a scalable manner under the honest-but-curious model. Our system provides the same level of scalability as a trusted central party based solu- tion while providing privacy guarantees without the need for heavyweight cryptography. The key idea is to develop an alternative system model using a Two-Party Query Com- putation Model comprising of a randomizer and a comput- ing engine which do not reveal any information between themselves. We also show how one can replace the ran- domizer by a lightweight key-agreement protocol. We for- mally prove the privacy-preserving properties of our proto- cols and demonstrate the scalability and practicality of our system using a real-world implementation.  1  
 SybilInfer is an algorithm for labelling nodes in a so- cial network as honest users or Sybils controlled by an adversary. At the heart of SybilInfer lies a probabilis- tic model of honest social networks, and an inference engine that returns potential regions of dishonest nodes. The Bayesian inference approach to Sybil detection comes with the advantage label has an assigned probability, indi- cating its degree of certainty. We prove through analytical results as well as experiments on simulated and real-world network topologies that, given standard constraints on the adversary, SybilInfer is secure, in that it successfully dis- tinguishes between honest and dishonest nodes and is not susceptible to manipulation by the adversary. Further- more, our results show that SybilInfer outperforms state of the art algorithms, both in being more widely applica- ble, as well as providing vastly more accurate results.  1  
 We present Spectrogram, a machine learning based statistical anomaly detection (AD) sensor for defense against web-layer code-injection attacks. These attacks include PHP ﬁle inclusion, SQL-injection and cross-site- scripting; memory-layer exploits such as buffer overﬂows are addressed as well. Statistical AD sensors offer the ad- vantage of being driven by the data that is being protected and not by malcode samples captured in the wild. While models using higher order statistics can often improve ac- curacy, trade-offs with false-positive rates and model ef- ﬁciency remain a limiting usability factor. This paper presents a new model and sensor framework that offers a fa- vorable balance under this constraint and demonstrates im- provement over some existing approaches. Spectrogram is a network situated sensor that dynamically assembles packets to reconstruct content ﬂows and learns to recog- nize legitimate web-layer script input. We describe an efﬁ- cient model for this task in the form of a mixture of Markov- chains and derive the corresponding training algorithm. Our evaluations show signiﬁcant detection results on an ar- ray of real world web layer attacks, comparing favorably against other AD approaches.  1 
 Several off-the-shelf products enable network operators to enforce usage restrictions by actively terminating con- nections when deemed undesirable. While the spectrum of their application is large—from ISPs limiting the usage of P2P applications to the “Great Firewall of China”—many of these systems implement the same approach to disrupt the communication: they inject artiﬁcial TCP Reset (RST) packets into the network, causing the endpoints to shut down communication upon receipt. In this work, we study the characteristics of packets injected by such trafﬁc con- trol devices. We show that by exploiting the race-conditions that out-of-band devices inevitably face, we not only can detect the interference but often also ﬁngerprint the spe- ciﬁc device in use. We develop an efﬁcient injection detector and demonstrate its effectiveness by identifying a range of disruptive activity seen in traces from four different sites, including termination of P2P connections, anti-spam and anti-virus mechanisms, and the ﬁnding that China’s “Great Firewall” has multiple components, sometimes apparently operating without coordination. We also ﬁnd a number of sources of idiosyncratic connection termination that do not reﬂect third-party trafﬁc disruption, including NATs, load- balancers, and spam bots. In general, our ﬁndings highlight that (i) Internet trafﬁc faces a wide range of control devices using injected RST packets, and (ii) to reliably detect RST injection while avoiding misidentiﬁcation of other types of activity requires signiﬁcant care.  1  
 Coordinated attacks, where the tasks involved in an at- tack are distributed amongst multiple sources, can be used by an adversary to obfuscate his incursion. In this paper we present an approach to detecting coordinated attacks that is based on adversary modeling of the desired information gain. A detection algorithm is developed that is based on solutions to the set covering problem, where we aim to rec- ognize coordinated activity by combining events such that a large portion of the information space is covered with mini- mal overlap. We demonstrate this approach by developing a coordinated scan detector, where the targets of a port scan are distributed amongst multiple coordinating sources. In this case, the adversary wishes to gain information about the active hosts and ports on a particular network. We pro- vide an algorithm that is capable of detecting horizontal and strobe scans against contiguous address spaces. We present experimental results from testing this algorithm in a controlled environment, demonstrating that it has an ac- ceptably low false positive rate, discussing the conditions required to maximize the detection rate and the limitations of the approach.  1  
 A Redirection Botnet (RBnet) is a vast collection of compromised computers (called bots) used as a redirec- tion/proxy infrastructure and under the control of a botmas- ter. We present the design, implementation and evaluation of a system called Redirection Botnet Seeker (RB-Seeker) for automatic detection of RBnets by utilizing three cooper- ating subsystems. Two of the subsystems are used to gen- erate a database of domains participating in redirection: one detects redirection bots by following links embedded in spam emails, and the other detects redirection behavior based on network traces at a large university edge router using sequential hypothesis testing. The database of redi- rection domains generated by these two subsystems is fed into the ﬁnal subsystem, which then performs DNS query probing on the domains over time. Based on certain behav- ioral attributes extracted from the DNS queries, the ﬁnal subsystem makes use of a 2-tier detection strategy utiliz- ing hyperplane decision functions. This allows it to quickly identify aggressive RBnets with a low false-positive rate (< 0.008%), while also accurately detecting stealthy RBnets (i.e., those mimicking valid DNS behavior, such as CDNs) by monitoring their behavior over time. Using DNS behav- ior as a means of detecting RBnets, RB-Seeker is impervious to the botmaster’s choice of Command-and-Control (C&C) channel (i.e., how the botmaster communicates and controls the bots) or use of encryption.  1  
Anti-malware companies receive thousands of malware samples every day. To process this large quantity, a number of automated analysis tools were developed. These tools execute a malicious program in a controlled environment and produce reports that summarize the program’s actions. Of course, the problem of analyzing the reports still remains. Recently, researchers have started to explore automated clustering techniques that help to identify samples that exhibit similar behavior. This allows an analyst to discard reports of samples that have been seen before, while focusing on novel, interesting threats. Unfortunately, previous techniques do not scale well and frequently fail to generalize the observed activity well enough to recognize related malware. In this paper, we propose a scalable clustering approach to identify and group malware samples that exhibit similar behavior. For this, we first perform dynamic analysis to obtain the execution traces of malware programs. These execution traces are then generalized into behavioral profiles, which characterize the activity of a program in more abstract terms. The profiles serve as input to an efficient clustering algorithm that allows us to handle sample sets that are an order of magnitude larger than previous approaches. We have applied our system to real-world malware collections. The results demonstrate that our technique is able to recognize and group malware programs that behave similarly, achieving a better precision than previous approaches. To underline the scalability of the system, we clustered a set of more than 75 thousand samples in less than three hours.
 Linking network ﬂows is an important problem in intru- sion detection as well as anonymity. Passive trafﬁc analysis can link ﬂows but requires long periods of observation to reduce errors. Watermarking techniques allow for better precision and blind detection, but they do so by introducing signiﬁcant delays to the trafﬁc ﬂow, enabling attacks that detect and remove the mark, while at the same time slowing down legitimate trafﬁc. We propose a new, non-blind water- marking scheme called RAINBOW that is able to use delays hundreds of times smaller than existing watermarks by elim- inating the interference caused by the ﬂow in the blind case. As a result, our watermark is invisible to detection, as con- ﬁrmed by experiments using information-theoretic detection tools.  We analyze the error rates of our scheme based on a mathematical model of network trafﬁc and jitter. We also validate the analysis using an implementation running on PlanetLab. We ﬁnd that our scheme generates orders of magnitudes lower rates of false errors than passive trafﬁc analysis, while using only a few hundred observed pack- ets. We also extend our scheme so that it is robust to packet drops and repacketization and show that ﬂows can still be reliably linked, though at the cost of somewhat longer ob- servation periods.  1  
 Recent work has shown that properties of network trafﬁc that remain observable after encryption, namely packet sizes and timing, can reveal surprising informa- tion about the trafﬁc’s contents (e.g., the language of a VoIP call [29], passwords in secure shell logins [20], or even web browsing habits [21, 14]). While there are some legitimate uses for encrypted trafﬁc analysis, these techniques also raise important questions about the pri- vacy of encrypted communications. A common tactic for mitigating such threats is to pad packets to uniform sizes or to send packets at ﬁxed timing intervals; however, this approach is often inefﬁcient. In this paper, we propose a novel method for thwarting statistical trafﬁc analysis algorithms by optimally morphing one class of trafﬁc to look like another class. Through the use of convex op- timization techniques, we show how to optimally modify packets in real-time to reduce the accuracy of a variety of trafﬁc classiﬁers while incurring much less overhead than padding. Our evaluation of this technique against two published trafﬁc classiﬁers for VoIP [29] and web trafﬁc [14] shows that morphing works well on a wide range of network data—in some cases, simultaneously providing better privacy and lower overhead than na¨ıve defenses.  1  
 DNS implementers face numerous choices in architecting DNS resolvers, each with profound implications for security. Absent the use of DNSSEC, there are numerous interim tech- niques to improve DNS forgery resistance. We explore how different resolver architectures can affect the risk of DNS poi- soning.  The contributions of this work include: (A) We create a comprehensive, accurate model of DNS poisoning. We show how this model is more sensitive than other previous ex- planations of DNS poisoning. (B) We further catalog the major architectural choices DNS implementers can make in query management. We note real-world instances where these choices have weakened the security of resolvers, and mea- sure the impact on security using our model. Our study re- vealed numerous, previously unknown vulnerabilities in com- mon DNS servers.  1 
 The number of identiﬁed integer overﬂow vulnerabilities has been increasing rapidly in recent years. In this paper, we present a system, IntScope, which can automatically de- tect integer overﬂow vulnerabilities in x86 binaries before an attacker does, with the goal of ﬁnally eliminating the vul- nerabilities. IntScope ﬁrst translates the disassembled code into our own intermediate representation (IR), and then per- forms a path sensitive data ﬂow analysis on the IR by lever- aging symbolic execution and taint analysis to identify the vulnerable point of integer overﬂow. Compared with other approaches, IntScope does not run the binary directly, and is scalable to large software as it can just symbolically ex- ecute the interesting program paths. Experimental results show IntScope is quite encouraging: it has detected more than 20 zero-day integer overﬂows (e.g., CVE-2008-4201, FrSIRT/ADV-2008-2919) in widely-used software such as QEMU, Xen and Xine.  1. 
 The prevalence of malware such as keyloggers and screen scrapers has made the prospect of providing sensitive infor- mation via web pages disconcerting for security-conscious users. We present Bumpy, a system to exclude the legacy operating system and applications from the trusted com- puting base for sensitive input, without requiring a hyper- visor or VMM. Bumpy allows the user to specify strings of input as sensitive when she enters them, and ensures that these inputs reach the desired endpoint in a protected state. The inputs are processed in an isolated code module on the user’s system, where they can be encrypted or otherwise processed for a remote webserver. We present a prototype implementation of Bumpy.  1  
 We introduce the notion of a conditioned-safe ceremony. A “ceremony” is similar to the conventional notion of a pro- tocol, except that a ceremony explicitly includes human par- ticipants. Our formulation of a conditioned-safe ceremony draws on several ideas and lessons learned from the human factors and human reliability community: forcing functions, defense in depth, and the use of human tendencies, such as rule-based decision making. We propose design principles for building conditioned-safe ceremonies and apply these principles to develop a registration ceremony for machine authentication based on email. We evaluated our email reg- istration ceremony with a user study of 200 participants. We designed our study to be as ecologically valid as possible: we employed deception, did not use a laboratory environ- ment, and attempted to create an experience of risk. We simulated attacks against the users and found that email registration was signiﬁcantly more secure than challenge question based registration. We also found evidence that conditioning helped email registration users resist attacks, but contributed towards making challenge question users more vulnerable.  1 
 We describe CSAR, a novel technique for generating cryp- tographically strong, accountable randomness. Using CSAR, we can generate a pseudo-random sequence and a proof that the elements of this sequence up to a given point have been correctly generated, while future values in the sequence remain unpredictable. CSAR enables ac- countability for distributed systems that use randomized protocols. External auditors can check if a node has devi- ated from its expected behavior without learning anything about the node’s future random choices. In particular, an accountable node does not need to leak secrets that would make its future actions predictable. We demonstrate that CSAR is practical and efﬁcient, and we apply it to imple- ment accountability for a server that uses random sam- pling for billing purposes.  1 
We introduce the notion of a conditioned-safe ceremony. A “ceremony” is similar to the conventional notion of a protocol, except that a ceremony explicitly includes human participants. Our formulation of a conditioned-safe ceremony draws on several ideas and lessons learned from the human factors and human reliability community: forcing functions, defense in depth, and the use of human tendencies, such as rule-based decision making. We propose design principles for building conditioned-safe ceremonies and apply these principles to develop a registration ceremony for machine authentication based on email. We evaluated our email registration ceremony with a user study of 200 participants. We designed our study to be as ecologically valid as possible: we employed deception, did not use a laboratory environment, and attempted to create an experience of risk. We simulated attacks against the users and found that email registration was significantly more secure than challenge question based registration. We also found evidence that conditioning helped email registration users resist attacks, but contributed towards making challenge question users more vulnerable.
We describe CSAR, a novel technique for generating cryptographically strong, accountable randomness. Using CSAR, we can generate a pseudo-random sequence and a proof that the elements of this sequence up to a given point have been correctly generated, while future values in the sequence remain unpredictable. CSAR enables accountability for distributed systems that use randomized protocols. External auditors can check if a node has deviated from its expected behavior without learning anything about the node’s future random choices. In particular, an accountable node does not need to leak secrets that would make its future actions predictable. We demonstrate that CSAR is practical and efficient, and we apply it to implement accountability for a server that uses random sampling for billing purposes.