 Malicious spyware poses a signiﬁcant threat to desktop security and integrity. This paper examines that threat from an Internet perspective. Using a crawler, we performed a large-scale, longitudinal study of the Web, sampling both executables and conventional Web pages for malicious ob- jects. Our results show the extent of spyware content. For example, in a May 2005 crawl of 18 million URLs, we found spyware in 13.4% of the 21,200 executables we identiﬁed. At the same time, we found scripted “drive-by download” attacks in 5.9% of the Web pages we processed. Our analy- sis quantiﬁes the density of spyware, the types of of threats, and the most dangerous Web zones in which spyware is likely to be encountered. We also show the frequency with which speciﬁc spyware programs were found in the content we crawled. Finally, we measured changes in the density of spyware over time; e.g., our October 2005 crawl saw a substantial reduction in the presence of drive-by download attacks, compared with those we detected in May.  1  
Internet attacks that use malicious web sites to install malware programs by exploiting browser vulnerabilities are a serious emerging threat. In response, we have developed an automated web patrol system to automatically identify and monitor these malicious sites. We describe the design and implementation of the Strider HoneyMonkey Exploit Detection System, which consists of a pipeline of “monkey programs” running possibly vulnerable browsers on virtual machines with different patch levels and patrolling the Web to seek out and classify web sites that exploit browser vulnerabilities. Within the first month of utilizing this system, we identified 752 unique URLs hosted on 288 web sites that could successfully exploit unpatched Windows XP machines. The system automatically constructed topology graphs based on traffic redirection to capture the relationship between the exploit sites. This allowed us to identify several major players who are responsible for a large number of exploit pages. By monitoring these 752 exploit-URLs on a daily basis, we discovered a malicious web site that was performing zero-day exploits of the unpatched javaprxy.dll vulnerability and was operating behind 25 exploit-URLs. It was confirmed as the first “inthe-wild”, zero-day exploit of this vulnerability that was reported to the Microsoft Security Response Center. Additionally, by scanning the most popular one million URLs as classified by a search engine, we found over seven hundred exploit-URLs, many of which serve popular content related to celebrities, song lyrics, wallpapers, video game cheats, and wrestling. 
 The custom, ad hoc nature of web applications makes learning-based anomaly detection systems a suitable approach to provide early warning about the exploita- tion of novel vulnerabilities. However, anomaly-based systems are known for producing a large number of false positives and for providing poor or non-existent infor- mation about the type of attack that is associated with an anomaly.  This paper presents a novel approach to anomaly- based detection of web-based attacks. The approach uses an anomaly generalization technique that automat- ically translates suspicious web requests into anomaly signatures. These signatures are then used to group re- current or similar anomalous requests so that an admin- istrator can easily deal with a large number of similar alerts.  In addition, the approach uses a heuristics-based technique to infer the type of attacks that generated the anomalies. This enables the prioritization of the at- tacks and provides better information to the adminis- trator. Our approach has been implemented and eval- uated experimentally on real-world data gathered from web servers at two universities.  1. 
When dealing with malware infections, one of the first tasksis to find the processes that were involved in the attack. We introduce Malfor, a system that isolates those processes automatically. In contrast to other methods that help analyze attacks, Malfor works by experiments: first, we record the interaction of the system under attack; after the intrusion has been detected, we replay the recorded events in slightly different configurationsto see which processes were relevant for the intrusion. This approach has three advantages over deductive approaches: first, the processes that are thus found have been experimentally shown to be relevant for the attack; second, the amount of evidence that must then be analyzed to find the attack vector is greatly reduced; and third, Malfor itself cannot make wrong deductions. In a first experiment, Malfor was able to extract the three processes responsible for an attack from 32 candidates in about six minutes.
 Exploits for new vulnerabilities, especially when incor- porated within a fast spreading worm, can compromise nearly all vulnerable hosts within a short amount of time. This problem demonstrates the need for fast defenses which can react to a new vulnerability quickly. In addition, a real- istic defense system should (a) not require source code since in practice most vulnerable systems do not have source code access nor is there adequate time to involve the software vendor, (b) be accurate, i.e., have a negligible false positive rate and low false negative rate, and (c) be efﬁcient, i.e., add little overhead to normal program execution.  We propose vulnerability-speciﬁc execution-based ﬁlter- ing (VSEF) – a new approach for automatic defense which achieves a lower error rate and wider applicability than in- put ﬁlters and has better performance than full execution monitoring. VSEF is an execution-based ﬁlter which ﬁlters out attacks on a speciﬁc vulnerability based on the vulnera- ble program’s execution trace. We present VSEF, along with a system for automatically creating VSEF ﬁlters and a hard- ened program without access to source code. In our system, the time it takes to create the ﬁlter and generate the hard- ened program is negligible. The overhead of the hardened program is only a few percent in most cases. The false pos- itive rate is zero in most cases, and the hardened program is resilient against polymorphic variants of exploits on the same vulnerability. VSEF therefore achieves the required performance, accuracy, and response speed requirements to defend against current fast-spreading exploits.  1. 
 Software monocultures are usually considered danger- ous because their size and uniformity represent the poten- tial for costly and widespread damage. The emerging con- cept of collaborative security provides the opportunity to re-examine the utility of software monoculture by exploit- ing the homogeneity and scale that typically deﬁne large software monocultures. Monoculture can be leveraged to improve an application’s overall security and reliability. We introduce and explore the concept of Application Com- munities: collections of large numbers of independent in- stances of the same application. Members of an applica- tion community share the burden of monitoring for ﬂaws and attacks, and notify the rest of the community when such are detected. Appropriate mitigation mechanisms are then deployed against the newly discovered fault. We explore the concept of an application community and determine its fea- sibility through analytical modeling and a prototype imple- mentation focusing on software faults and vulnerabilities. Speciﬁcally, we identify a set of parameters that deﬁne application communities and explore the tradeoffs between the minimal size of an application community, the marginal overhead imposed on each member, and the speed with which new faults are detected and isolated. We demon- strate the feasibility of the scheme using Selective Trans- actional EMulation (STEM) as both the monitoring and remediation mechanism for low-level software faults, and provide some preliminary experimental results using the Apache web server as the protected application. Our ex- periments show that ACs are practical and feasible for cur- rent applications: an AC of 15,000 members can collabo- ratively monitor Apache for new faults and immunize all members against them with only a 6% performance degra- dation for each member.  1 
Address harvesting is the act of searching a compromised host for the names and addresses of other targets to attack, such as occurs when an email virus locates target addresses from users’ address lists or mail archives. We examine how host addresses harvested from Secure Shell (SSH) clients’ known hosts files can aid those attacking SSH servers. Each user’s known hosts file contains the names of every host previously accessed by its owner. Thus, when an attacker compromises a user’s password or identity key, the known hosts file can be used to identify those hosts on a network that are most likely to accept this compromised credential. Such attacks are not theoretical – a single attacker who targeted host authentication via SSH and employed known hosts address harvesting was able to gain access to a multitude of academic, commercial, and government systems. To show the value of known hosts files to such attackers, we present results of a study of known hosts files and other data collected from 173 hosts distributed over 25 top level domains. We also collected data on users’ credential management practices, and discovered that 61.7% of the identity keys we encountered were stored unencrypted. To show how host authentication attacks via SSH could evolve if automated, we survey mechanisms used to attack and their suitability for use in self-propagating code. Finally, we present countermeasures devised to defend against address harvesting, which have been adopted by the OpenSSH team and one of the two main commercial SSH software vendors.
Enterprise networks today carry a range of mission critical communications. A successful worm attack within an enterprise network can be substantially more devastating to most companies than attacks on the larger Internet. In this paper we explore a brownfield approach to hardening an enterprise network against active malware such as worms. The premise of our approach is that if future communication patterns are constrained to historical “normal” communication patterns, then the ability of malware to exploit vulnerabilities in the enterprise can be severely curtailed. We present techniquesfor automatically deriving individual host profiles that capture historical communication patterns (i.e., community of interest (COI)) of end hosts within an enterprise network. Using traces from a large enterprise network, we investigate how a range of different security policies based on these profiles impact usability (as valid communications may get restricted) and security (how well the policies contain malware). Our evaluations indicate that a simple security policy comprised of our Extended COIbased profile and Relaxed Throttling Discipline can effectively contain worm behavior within an enterprise without significantly impairing normal network operation.
For many applications—including recognizing malware variants, determining the range of system versions vulnerable to a given attack, testing defense mechanisms, and filtering multi-step attacks—it can be highly useful to mimic an existing system while interacting with a live host on the network. We present RolePlayer, a system which, given examples of an application session, can mimic both the client side and the server side of the session for a wide variety of application protocols. A key property of RolePlayer is that it operates in an application-independent fashion: the system does not require any specifics about the particular application it mimics. It instead uses byte-stream alignment algorithms to compare different instances of a session to determine which fields it must change to successfully replay one side of the session. Drawing only on knowledge of a few low-level syntactic conventions (such as representing IP addresses using “dotted quads”), and contextual information such as the domain names of the participating hosts, RolePlayer can heuristically detect and adjust network addresses, ports, cookies, and length fields embedded within the session, including sessions that span multiple, concurrent connections on dynamically assigned ports. We have successfully used RolePlayer to replay both the client and server sides for a variety of network applications, including NFS, FTP, and CIFS/SMB file transfers, as well as the multi-stage infection processes of the Blaster and W32.Randex.D worms.
 a substantial extension of the state-of-the-art in privacy- preserving trust negotiations.  In an open environment such as the Internet, the decision to collaborate with a stranger (e.g., by granting access to a resource) is often based on the characteristics (rather than the identity) of the requester, via digital credentials: Access is granted if Alice’s credentials satisfy Bob’s access policy. The literature contains many examples where protecting the credentials and the access control policies is useful, and there are numerous protocols that achieve this. In many of these schemes, the server does not learn whether the client obtained access (e.g., to a message, or a service via an e- ticket). A consequence of this property is that the client can use all of her credentials without fear of “probing” attacks by the server, because the server cannot glean information about which credentials the client has (when this property is lacking, the literature uses a framework where the very use of a credential is subject to a policy speciﬁc to that creden- tial). The main result of this paper is a protocol for negoti- ating trust between Alice and Bob without revealing either credentials or policies, when each credential has its own ac- cess policy associated with it (e.g., “a top-secret clearance credential can only be used when the other party is a gov- ernment employee and has a top-secret clearance”). Our protocol carries out this privacy-preserving trust negotia- tion between Alice and Bob, while enforcing each creden- tial’s policy (thereby protecting sensitive credentials). Note that there can be a deep nesting of dependencies between credential policies, and that there can be (possibly over- lapping) policy cycles of these dependencies. Our result is not achieved through the routine use of standard tech- niques to implement, in this framework, one of the known strategies for trust negotiations (such as the “eager strat- egy”). Rather, this paper uses novel techniques to imple- ment a non-standard trust negotiation strategy speciﬁcally suited to this framework (and in fact unusable outside of this framework, as will become clear). Our work is therefore  ∗Portions of this work were supported by Grants IIS-0325345, IIS- 0219560, IIS-0312357, and IIS-0242421 from the National Science Foun- dation, Contract N00014-02-1-0364 from the Ofﬁce of Naval Research, by sponsors of the Center for Education and Research in Information Assur- ance and Security, and by Purdue Discovery Park’s e-enterprise Center.  1  
 The strength of hash functions such as MD5 and SHA-1 has been called into question as a result of recent discov- eries. Regardless of whether or not it is necessary to move away from those now, it is clear that it will be necessary to do so in the not-too-distant future. This poses a number of challenges, especially for certiﬁcate-based protocols. We analyze a number of protocols, including S/MIME and TLS. All require protocol or implementation changes. We explain the necessary changes, show how the conversion can be done, and list what measures should be taken immediately.  However, it is clear that neither MD5 nor SHA-1 is as strong as its target security level and so need to be replaced. The possibility of new attacks lends some urgency to this transi- tion.  Although we don’t discuss the issue in detail, most of our analysis applies to deploying new signature algorithms as well as to deploying new hash functions. If the signature algorithm is linked to a particular hash function, as DSA is tied to SHA-1, the two would change together; beyond that, since signature algorithms are almost always applied to the output of hash functions, if there is no easy way to substitute a new hash algorithm there is almost certainly no way to substitute a new signature algorithm, either.  1 
 The Plutus ﬁle system introduced the notion of key rotation as a means to derive a sequence of temporally- related keys from the most recent key. In this paper we show that, despite natural intuition to the contrary, key rotation schemes cannot generically be used to key other cryptographic objects; in fact, keying an encryp- tion scheme with the output of a key rotation scheme can yield a composite system that is insecure. To address these shortcomings, we introduce a new cryptographic object called a key regression scheme, and we propose three constructions that are provably secure under stan- dard cryptographic assumptions. We implement key re- gression in a secure ﬁle system and empirically show that key regression can signiﬁcantly reduce the band- width requirements of a content publisher under real- istic workloads using lazy revocation. Our experiments also serve as the ﬁrst empirical evaluation of either a key rotation or key regression scheme.  Keywords: Key regression, key rotation, lazy revoca- tion, key distribution, content distribution network, hash chain, security proofs.  1 
Structured overlays are an important and powerful class of overlay networks that has emerged in recent years. They are typically targeted at peer-to-peer deployments involving millions of user-managed machines on the Internet. In this paper we address routing-table poisoning attacks against structured overlays, in which adversaries attempt to intercept traffic and control the system by convincing other nodes to use compromised nodes as their overlay network neighbors. In keeping with the fully-decentralized goals of structured overlay design, we propose a defense mechanism that makes minimal use of centralized infrastructure. Our approach, induced churn, utilizes periodic routing-table resets, unpredictable identifier changes, and a rate limit on routing-table updates. Induced churn leaves adversaries at the mercy of chance: they have little opportunity to strategize their positions in the overlay, and cannot entrench themselves in any position that they do acquire. We implement induced churn in Maelstrom, an extension to the broadly used Bamboo distributed hash table. Our Maelstrom experiments over a simulated network demonstrate robust routing with very modest costs in bandwidth and latency, at levels of adversarial activity where unprotected overlays are rendered almost completely useless.
 As part of  the Detecting Intrusions at Layer ONe (DILON) project, we show that Ethernet devices can be uniquely identiﬁed and tracked—using as few as 25 Ether- net frames—by analyzing variations in their analog signal caused by hardware and manufacturing inconsistencies. An optimal detector, the matched ﬁlter, is utilized to create sig- nal proﬁles, which aid in identifying the device the signal originated from. Several non-traditional applications of the ﬁlter are presented in order to improve its ability to dis- criminate between signals from seemingly identical devices of the same manufacturing lot. The experimental results of applying these ﬁlters to three different models of Ethernet cards, totaling 16 devices, are presented and discussed.  Important applications of this technology include intru- sion detection (discovering node impersonation and net- work tampering), authentication (preventing unauthorized access to the physical network), forensic data collection (ty- ing a physical device to a speciﬁc network incident), and assurance monitoring (determining whether a device will or is in the process of failing).  1. 
 Time zones play an important and unexplored role in malware epidemics. To understand how time and loca- tion affect malware spread dynamics, we studied botnets, or large coordinated collections of victim machines (zom- bies) controlled by attackers. Over a six month period we observed dozens of botnets representing millions of vic- tims. We noted diurnal properties in botnet activity, which we suspect occurs because victims turn their computers off at night. Through binary analysis, we also con(cid:2)rmed that some botnets demonstrated a bias in infecting regional pop- ulations.  Clearly, computers that are of(cid:3)ine are not infectious, and any regional bias in infections will affect the overall growth of the botnet. We therefore created a diurnal propagation model. The model uses diurnal shaping functions to capture regional variations in online vulnerable populations.  The diurnal model also lets one compare propagation rates for different botnets, and prioritize response. Because of variations in release times and diurnal shaping functions particular to an infection, botnets released later in time may actually surpass other botnets that have an advanced start. Since response times for malware outbreaks is now mea- sured in hours, being able to predict short-term propagation dynamics lets us allocate resources more intelligently. We used empirical data from botnets to evaluate the analytical model.  1 
 Volunteer distributed computations utilize spare proces- sor cycles of personal computers that are connected to the Internet. The resulting platforms provide computational power previously available only through the use of ex- pensive clusters or supercomputers. However, distributed computations running in untrustworthy environments raise a number of security concerns, including computation in- tegrity and data privacy.  This paper introduces a strategy for enhancing data pri- vacy in some distributed volunteer computations, providing an important ﬁrst step toward a general data privacy so- lution for these computations. The strategy is used to provide enhanced data privacy for the Smith-Waterman local nucleotide sequence comparison algorithm. Our modiﬁed Smith-Waterman algorithm provides reasonable performance, identifying most, and in many cases all, se- quence pairs that exhibit statistically signiﬁcant similarity according to the unmodiﬁed algorithm, with reasonable levels of false positives. Moreover the modiﬁed algorithm achieves a net decrease in execution time, with no increase in memory requirements. Most importantly, our scheme represents an important ﬁrst step toward providing data privacy for a practical and important real-world algorithm.  Keywords:  distributed computation, data privacy,  Smith-Waterman algorithm  1. 
We provide a largely automated system for verifying ClarkWilson interprocess information-flow integrity. Information-flow integrity properties are essential to isolate trusted processes from untrusted ones, but system misconfiguration can easily create insecure dependences. For example, an untrusted user process may be able to write to sshd config via a cron script. A useful notion of integrity is the Clark-Wilson integrity model [7], which allows trusted processes to accept necessary untrusted inputs (e.g., network data or print jobs) via filtering interfaces that sanitize the data. However, Clark-Wilson has the requirement that programs undergo formal semantic verification; in practice, this kind of burden has meant that no information-flow integrity property is verified on most widely-used systems. We define a weaker version of Clark-Wilson integrity, called CW-Lite, which has the same interprocess information-flow guarantees, but which requires less filtering, only small changes to existing applications, and which we can check using automated tools. We modify the SELinux user library and kernel module in order to support CW-Lite integrity verification and develop new software tools to aid developers in finding and enabling filtering interfaces. Using our toolset, we found and fixed several integrity-violating configuration errors in the default SELinux policies for OpenSSH and vsftpd.
