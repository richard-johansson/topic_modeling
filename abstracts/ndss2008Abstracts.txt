Although several wide-spread internet applications (e.g., job-referral services, dating services) can benefit from online matchmaking, protocols defined over the past two decades fail to address important privacy concerns. In this paper, we enhance traditional privacy requirements (e.g., user anonymity, matching-wish authenticity) with new privacy goals (e.g., resistance to off-line dictionary attacks, and forward privacy of users’ identities and matching wishes), and argue that privacy-enhanced matchmaking cannot be provided by solutions to seemingly related problems such as secret handshakes, set intersection, and trust negotiation. We define an adversary model, which captures the key security properties of privacy-enhanced matchmaking, and show that a simple, practical protocol derived by a two-step transformation of a password-based authenticated key exchange counters adversary attacks in a provable manner (in the standard model of cryptographic security)
 We study and document an important development in how attackers are using Internet resources: the creation of mali- cious DNS resolution paths. In this growing form of attack, victims are forced to use rogue DNS servers for all resolu- tion. To document the rise of this “second secret authority” on the Internet, we studied instances of aberrant DNS resolu- tion on a university campus. We found dozens of viruses that corrupt resolution paths, and noted that hundreds of URLs dis- covered per week performed drive-by alterations of host DNS settings. We used the rogue servers discovered in this analy- sis to document numerous live incidents on the university net- work. To measure this problem on the larger Internet, we gen- erated DNS requests to most of IPv4, using a unique label query for each request. We found 17 million hosts responding, and further tracked the resolution path they used to reach our NS. Unable to ﬁnd plausible harmless explanations for such a large number of open recursive hosts, we queried 600,000 of these open resolvers for “phishable” domains, such as banks and anti-virus companies. We found that 2.4% of this subsam- ple would reply with incorrect answers, which extrapolates to 291,528 hosts on the Internet performing either incorrect or malicious DNS service. With DNS resolution behavior so triv- ially changed, numerous malware instances in the wild, and so many other hosts providing incorrect and misleading answers, we urge the security community to consider the corruption of the resolution path as an important problem.  1  
 The Tor anonymous communication network uses self- reported bandwidth values to select routers for building tunnels. Since tunnels are allocated in proportion to this bandwidth, this allows a malicious router operator to at- tract tunnels for compromise. Since the metric used is in- sensitive to relative load, it does not adequately respond to changing conditions and hence produces unreliable perfor- mance, driving many users away. We propose an oppor- tunistic bandwidth measurement algorithm to replace self- reported values and address both of these problems. We also propose a mechanisms to let users tune Tor perfor- mance to achieve higher performance or higher anonymity. Our mechanism effectively blends the trafﬁc from users of different preferences, making partitioning attacks difﬁcult. We implemented the opportunistic measurement and tun- able performance extensions and examined their perfor- mance both analytically and in the real Tor network. Our results show that users can get dramatic increases in either performance or anonymity with little to no sacriﬁce in the other metric, or a more modest improvement in both. Our mechanisms are also invulnerable to the previously pub- lished low-resource attacks on Tor.  1. 
We study the problem of reliably searching for resources in untrusted peer-to-peer networks, where a significant portion of the participating network nodes may act maliciously to subvert the search process. We present a new method called Halo for performing redundant searches over a distributed hash table (DHT) structure to achieve high integrity and availability levels without affecting the storage and communication complexities of the underlying DHT. Other schemes for redundant searches have proposed new or modified DHTs with increased storage requirements at nodes, requiring modifications at all nodes in the network. In contrast, Halo aims to serve as a middleware component, making “black-box” calls of the underlying primitive search operation to eventually provide a new composite search operation of higher assurance. We apply this concept to the popular and well-studied DHT Chord, and demonstrate the efficiency and security of our approach though analytical modeling and simulation-based analysis. For example, we show that for 12% malicious nodes in the network, a regular Chord operation fails 50–60% of the time. In contrast, Halo reduces this failure rate to 1%. We show how our scheme lends itself to a recursive version that can tolerate 22% malicious nodes with the same level of success, while regular Chord fails 70–80% of the time.
For most elections, receipt-freeness is important – voters are unable to prove to others on how they voted, in order to prevent vote-buying. Many existing receipt-free electronic voting systems are not practical enough as they require voters to participate in the tallying phase (i.e. do not satisfy the vote-and-go requirement), or have no mechanism for the voters to verify whether their votes have been counted (i.e. do not satisfy universal verifiability). We propose a new way of constructing vote-and-go election system without tamper-resistant hardware, or anonymous channel. Receipt-freeness is guaranteed even if there is only one voting authority (in a distributed setting) being honest. Regarding the correctness, voter alone has no chance to tamper with the validity of the final tally, while any misbehaving authority can be detected (and proven to the public) by the tallying center. Robustness can be achieved by fixing the corrupted vote in a verifiable manner. Ballot secrecy cannot be compromised even if all tallying authorities collude
The enormous growth in the diversity of content services such as IPtv has highlighted the inadequacy of the accompanying content security: existing security mechanisms scale poorly, require complex and often costly dedicated hardware, or fail to meet basic security requirements. New security methods are needed. In this paper, we explore the ability of attribute-based encryption (ABE) to meet the unique performance and security requirements of conditional access systems such as subscription radio and payper-view television. We show through empirical study that costs of ABE make its direct application inappropriate, but present constructions that mitigate its incumbent costs. We develop an extensive simulation that allows us to explore the performance of a number of virtual hardware configurations and construction parameters over workloads developed from real subscription and television audiences. These simulations show that we can securely deliver high quality content to viewerships of the highest rated shows being broadcast today, some in excess of 26,000,000 viewers. It is through these experiments that we expose the viability of not only ABE-based content delivery, but applicability of ABE systems to large-scale distributed systems.
Accurate network measurement through trace collection is critical for advancing network design and for maintaining secure, reliable networks. Unfortunately, the release of network traces to analysts is highly constrained by privacy concerns. Several host anonymization schemes have been proposed to address this issue. Preservation of prefix relationships among anonymized addresses is an important aspect of trace utility, but also causes a number of vulnerabilities in trace anonymization. In this work we present an efficient host fingerprint attack targeting prefix-preserving anonymized traces. The attack is general (encompassing a range of fingerprinting host de-anonymization attacks proposed by others) and flexible (it can be adapted to emerging variants of prefix-preserving anonymization). Perhaps most importantly, we develop analysis tools that allow data publishers to quantify the worst-case vulnerability of their traces given assumptions about the kind of external information that is available to the adversary. Using this analysis we quantify the trade-off between privacy and utility of alternatives to full prefix-preserving anonymization.
 Anonymization plays a key role in enabling the pub- lic release of network datasets, and yet there are few, if any, techniques for evaluating the efﬁcacy of network data anonymization techniques with respect to the pri- vacy they afford. In fact, recent work suggests that many state-of-the-art anonymization techniques may In this pa- leak more information than ﬁrst thought. per, we propose techniques for evaluating the anonymity of network data. Speciﬁcally, we simulate the behavior of an adversary whose goal is to deanonymize objects, such as hosts or web pages, within the network data. By doing so, we are able to quantify the anonymity of the data using information theoretic metrics, objectively compare the efﬁcacy of anonymization techniques, and examine the impact of selective deanonymization on the anonymity of the data. Moreover, we provide several concrete applications of our approach on real network data in the hope of underscoring its usefulness to data publishers.  1  
 In [22] we showed that existing single-server computa- tional private information retrieval (PIR) protocols for the purpose of preserving client access patterns leakage are or- ders of magnitude slower than trivially transferring the en- tire data sets to the inquiring clients. We thus raised the issue of designing efﬁcient PIR mechanisms in practical set- tings.  In this paper we introduce exactly such a technique, guaranteeing access pattern privacy against a computa- tionally bounded adversary, in outsourced data storage, with communication and computation overheads orders of In the pres- magnitude better than existing approaches. ence of a small amount (O(√n), where n is the size of the database) of temporary storage, clients can achieve access pattern privacy with communication and computa- tional complexities of less than O(log2n) per query (as compared to e.g., O(log4n) for existing approaches).  We achieve these novel results by applying new insights based on probabilistic analyses of data shufﬂing algorithms to Oblivious RAM [17], allowing us to signiﬁcantly improve its asymptotic complexity. This results in a protocol cross- ing the boundary between theory and practice and becom- ing generally applicable for access pattern privacy. We show that on off the shelf hardware, large data sets can be queried obliviously orders of magnitude faster than in exist- ing work.  1 
 Fuzz testing is an effective technique for ﬁnding security vulnerabilities in software. Traditionally, fuzz testing tools apply random mutations to well-formed inputs of a pro- gram and test the resulting values. We present an alterna- tive whitebox fuzz testing approach inspired by recent ad- vances in symbolic execution and dynamic test generation. Our approach records an actual run of the program un- der test on a well-formed input, symbolically evaluates the recorded trace, and gathers constraints on inputs capturing how the program uses these. The collected constraints are then negated one by one and solved with a constraint solver, producing new inputs that exercise different control paths in the program. This process is repeated with the help of a code-coverage maximizing heuristic designed to ﬁnd defects as fast as possible. We have implemented this algorithm in SAGE (Scalable, Automated, Guided Execution), a new tool employing x86 instruction-level tracing and emulation for whitebox fuzzing of arbitrary ﬁle-reading Windows ap- plications. We describe key optimizations needed to make dynamic test generation scale to large input ﬁles and long execution traces with hundreds of millions of instructions. We then present detailed experiments with several Windows applications. Notably, without any format-speciﬁc knowl- edge, SAGE detects the MS07-017 ANI vulnerability, which was missed by extensive blackbox fuzzing and static analy- sis tools. Furthermore, while still in an early stage of de- velopment, SAGE has already discovered 30+ new bugs in large shipped Windows applications including image pro- cessors, media players, and ﬁle decoders. Several of these bugs are potentially exploitable memory access violations.  1  
Today’s Internet routing infrastructure exhibits high homogeneity. This constitutes a serious threat to the resilience of the network, since a bug or security vulnerability in an implementation could make all routers running that implementation become simultaneously unusable. This situation could arise as a result of a defective software upgrade or a denial-of-service attack. Diversity has been proposed as a solution to increase resilience to software defects, but the benefits have not been clearly studied. In this paper, we use a graph theoretic approach to study the benefits of diversity for the robustness of a network, where robustness is the property of a network staying connected under a software failure. We address three fundamental questions: 1) How do we measure the robustness of a network under such failures? 2) How much diversity is needed to guarantee a certain degree of robustness? 3) Is there enough diversity already in the network or do we need to introduce more? We find that a small degree of diversity can provide good robustness. In particular, for a Tier-1 ISP network, five implementations suffice: two for the backbone routers and three for the access routers. We learn that some networks may already have enough diversity, but the diversity is not adequately used for robustness. We observe that the best way to apply diversity is to partition the network into contiguous regions using the same implementation, separating backbone and access routers and taking into account if a router is replicated. We evaluate our approach on multiple real ISP topologies, including the topology of a Tier-1 ISP
A grand challenge in information protection is how to preserve the confidentiality of sensitive information under spyware surveillance. This problem has not been well addressed by the existing access-control mechanisms which cannot prevent the spyware already in a system from monitoring an authorized party’s interactions with sensitive data. Our answer to this challenge is PRECIP, a new security policy model which takes a first step towards practical and retrofittable confidential information protection. This model is designed to offer efficient online protection for commercial applications and operating systems. It intends to be retrofitted to these applications and systems without modifying their code. To this end, PRECIP addresses several practical issues critical to containing spyware surveillance, which however are not well handled by the previous work in access control and information-flow security. Examples include the models for human input devices such as keyboard whose sensitivity level must be dynamically determined, other shared resources such as clipboard and screen which must be accessed by different processes, and the multitasked processes which work on public and sensitive data concurrently. We applied PRECIP to Windows XP to protect the applications for editing or viewing sensitive documents and browsing sensitive websites. We demonstrate that our implementation works effectively against a wide spectrum of spyware, including keyloggers, screen grabbers and file stealers. We also evaluated the overheads of our technique, which are shown to be very small.
 Protocol reverse engineering is the process of ex- tracting application-level speciﬁcations for network pro- tocols. Such speciﬁcations are very helpful in a number of security-related contexts. For example, they are needed by intrusion detection systems to perform deep packet in- spection, and they allow the implementation of black-box fuzzing tools. Unfortunately, manual reverse engineering is a time-consuming and tedious task. To address this prob- lem, researchers have recently proposed systems that help to automate the process. These systems operate by ana- lyzing traces of network trafﬁc. However, there is limited information available at the network-level, and thus, the accuracy of the results is limited.  In this paper, we present a novel approach to automatic protocol reverse engineering. Our approach works by dy- namically monitoring the execution of the application, an- alyzing how the program is processing the protocol mes- sages that it receives. This is motivated by the insight that an application encodes the complete protocol and repre- sents the authoritative speciﬁcation of the inputs that it can accept. In a ﬁrst step, we extract information about the ﬁelds of individual messages. Then, we aggregate this in- formation to determine a more general speciﬁcation of the message format, which can include optional or alternative ﬁelds, and repetitions. We have applied our techniques to a number of real-world protocols and server applications. Our results demonstrate that we are able to extract the for- mat speciﬁcation for different types of messages. Using these speciﬁcations, we then automatically generate ap- propriate parser code.  1 
Protocol reverse engineering has often been a manual process that is considered time-consuming, tedious and error-prone. To address this limitation, a number of solutions have recently been proposed to allow for automatic protocol reverse engineering. Unfortunately, they are either limited in extracting protocol fields due to lack of program semantics in network traces or primitive in only revealing the flat structure of protocol format. In this paper, we present a system called AutoFormat that aims at not only extracting protocol fields with high accuracy, but also revealing the inherently “non-flat”, hierarchical structures of protocol messages. AutoFormat is based on the key insight that different protocol fields in the same message are typically handled in different execution contexts (e.g., the runtime call stack). As such, by monitoring the program execution, we can collect the execution context information for every message byte (annotated with its offset in the entire message) and cluster them to derive the protocol format. We have evaluated our system with more than 30 protocol messages from seven protocols, including two text-based protocols (HTTP and SIP), three binary-based protocols (DHCP, RIP, and OSPF), one hybrid protocol (CIFS/SMB), as well as one unknown protocol used by a real-world malware. Our results show that AutoFormat can not only identify individual message fields automatically and with high accuracy (an average 93.4% match ratio compared with Wireshark), but also unveil the structure of the protocol format by revealing possible relations (e.g., sequential, parallel, and hierarchical) among the message fields
 We present the ﬁrst empirical study of fast-ﬂux service networks (FFSNs), a newly emerging and still not widely- known phenomenon in the Internet. FFSNs employ DNS to establish a proxy network on compromised machines through which illegal online services can be hosted with very high availability. Through our measurements we show that the threat which FFSNs pose is signiﬁcant: FFSNs oc- cur on a worldwide scale and already host a substantial percentage of online scams. Based on analysis of the prin- ciples of FFSNs, we develop a metric with which FFSNs can be effectively detected. Considering our detection technique we also discuss possible mitigation strategies.  1 
 Botnets are now recognized as one of the most serious security threats. In contrast to previous malware, botnets have the characteristic of a command and control (C&C) channel. Botnets also often use existing common protocols, e.g., IRC, HTTP, and in protocol-conforming manners. This makes the detection of botnet C&C a challenging problem. In this paper, we propose an approach that uses network-based anomaly detection to identify botnet C&C channels in a local area network without any prior knowl- edge of signatures or C&C server addresses. This detection approach can identify both the C&C servers and infected hosts in the network. Our approach is based on the observa- tion that, because of the pre-programmed activities related to C&C, bots within the same botnet will likely demonstrate spatial-temporal correlation and similarity. For example, they engage in coordinated communication, propagation, and attack and fraudulent activities. Our prototype system, BotSniffer, can capture this spatial-temporal correlation in network trafﬁc and utilize statistical algorithms to detect botnets with theoretical bounds on the false positive and false negative rates. We evaluated BotSniffer using many real-world network traces. The results show that BotSniffer can detect real-world botnets with high accuracy and has a very low false positive rate.  1 
Botnets are now recognized as one of the most serious security threats. In contrast to previous malware, botnets have the characteristic of a command and control (C&C) channel. Botnets also often use existing common protocols, e.g., IRC, HTTP, and in protocol-conforming manners. This makes the detection of botnet C&C a challenging problem. In this paper, we propose an approach that uses network-based anomaly detection to identify botnet C&C channels in a local area network without any prior knowledge of signatures or C&C server addresses. This detection approach can identify both the C&C servers and infected hosts in the network. Our approach is based on the observation that, because of the pre-programmed activities related to C&C, bots within the same botnet will likely demonstrate spatial-temporal correlation and similarity. For example, they engage in coordinated communication, propagation, and attack and fraudulent activities. Our prototype system, BotSniffer, can capture this spatial-temporal correlation in network traffic and utilize statistical algorithms to detect botnets with theoretical bounds on the false positive and false negative rates. We evaluated BotSniffer using many real-world network traces. The results show that BotSniffer can detect real-world botnets with high accuracy and has a very low false positive rate.
Automatic signature generation is necessary because there may often be little time between the discovery of a vulnerability, and exploits developed to target the vulnerability. Much research effort has focused on patternextraction techniques to generate signatures. These have included techniques that look for a single large invariant substring of the byte sequences, as well as techniques that look for many short invariantsubstrings. Pattern-extraction techniques are attractive because signatures can be generated and matched efficiently, and earlier work has shown the existence of invariants in exploits. In this paper, we show fundamental limits on the accuracy of pattern-extraction algorithms for signaturegeneration in an adversarial setting. We formulate a framework that allows a unified analysis of these algorithms, and prove lower bounds on the number of mistakes any patternextraction learning algorithm must make under common assumptions, by showing how to adapt results from learning theory. While previous work has targeted specific algorithms, our work generalizes these attacks through theoretical analysis to any algorithm with similar assumptions, not just the techniques developed so far. We also analyze when pattern-extraction algorithms may work, by showing conditions under which these lower bounds are weakened. Our results are applicable to other kinds ofsignature-generation algorithms as well, those that use properties of the exploit that can be manipulated.
 Traditionally, techniques for computing on encrypted data have been proposed with privacy preserving applica- tions in mind. Several current cryptosystems support a ho- momorphic operation, allowing simple computations to be performed using encrypted values. This is sufﬁcient to real- ize several useful applications, including schemes for elec- tronic voting [16, 12, 17] and single server private infor- mation retrieval (PIR) [19, 9]. In this paper, we introduce an alternative application for these techniques in an unex- pected setting: malware. We point out the counterintuitive possibility of malware which renders some aspects of its be- havior provably resistant to forensic analysis, even with full control over the malware code, its input, and its execution environment. While methods for general purpose computa- tion on encrypted data have not yet been realized, we ex- plore the potential use of current techniques. Speciﬁcally, we consider in depth the possibility of malware which em- ploys private information retrieval techniques to ﬁnd and retrieve speciﬁc pieces of sensitive information from com- promised hosts while hiding its search criteria. Through an evaluation of the goals of attackers and the constraints un- der which they operate, we determine that PIR techniques are an attractive technology to malware authors with the potential to increase the threat of targeted espionage. We go on to demonstrate the present feasibility of PIR-based mal- ware through a series of experiments with a full implemen- tation of a recent private stream searching scheme. Through the example of PIR-based malware, we highlight the more general possibilities of computing on encrypted data in a malicious setting.  1  
 We discovered two vulnerabilities in the PF sched-  Third Generation (3G) cellular networks utilize time- varying and location-dependent channel conditions to provide broadband services. They employ opportunis- tic scheduling to efﬁciently utilize spectrum under fair- ness or QoS constraints. Opportunistic scheduling al- gorithms rely on collaboration among all mobile users to achieve their design objectives. However, we demon- strate that rogue cellular devices can exploit vulnera- bilities in opportunistic scheduling algorithms, such as Proprotional Fair (PF), to usurp the majority of time slots in 3G networks. Our simulations show that only ﬁve rogue device per 50-user cell can use up to 90% of the time slots, and can cause 2 seconds of end-to-end inter-packet transmission delay on VoIP applications for every user in the same cell, rendering VoIP applications useless. To defend against these attacks, we explore sev- eral detection and prevention schemes, including modi- ﬁcations to the PF scheduler and a secure handoff pro- cedure.  1  
Third Generation (3G) cellular networks utilize timevarying and location-dependent channel conditions to provide broadband services. They employ opportunistic scheduling to efficiently utilize spectrum under fairness or QoS constraints. Opportunistic scheduling algorithms rely on collaboration among all mobile users to achieve their design objectives. However, we demonstrate that rogue cellular devices can exploit vulnerabilities in opportunistic scheduling algorithms, such as Proprotional Fair (PF), to usurp the majority of time slots in 3G networks. Our simulations show that only five rogue device per 50-user cell can use up to 90% of the time slots, and can cause 2 seconds of end-to-end inter-packet transmission delay on VoIP applications for every user in the same cell, rendering VoIP applications useless. To defend against these attacks, we explore several detection and prevention schemes, including modifications to the PF scheduler and a secure handoff procedure.